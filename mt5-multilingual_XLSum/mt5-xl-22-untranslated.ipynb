{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8184236,"sourceType":"datasetVersion","datasetId":4845936},{"sourceId":8204708,"sourceType":"datasetVersion","datasetId":4861189},{"sourceId":8215303,"sourceType":"datasetVersion","datasetId":4869275},{"sourceId":8220751,"sourceType":"datasetVersion","datasetId":4873549},{"sourceId":8220822,"sourceType":"datasetVersion","datasetId":4873602}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import csv\nimport pandas as pd \nimport torch\nimport torch.nn as nn\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:05.809301Z","iopub.execute_input":"2024-04-25T13:21:05.810109Z","iopub.status.idle":"2024-04-25T13:21:05.815967Z","shell.execute_reply.started":"2024-04-25T13:21:05.810077Z","shell.execute_reply":"2024-04-25T13:21:05.814693Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\n# s='train[:10]'\n# dataset_train = load_dataset('csv', data_files='/kaggle/input/translated-data/Translated_hindi_train.csv')\n# dataset_val = load_dataset('csv', data_files='/kaggle/input/summariser-data2022/val.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:05.818010Z","iopub.execute_input":"2024-04-25T13:21:05.818537Z","iopub.status.idle":"2024-04-25T13:21:05.825435Z","shell.execute_reply.started":"2024-04-25T13:21:05.818476Z","shell.execute_reply":"2024-04-25T13:21:05.824369Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"# dataset_train","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:05.827175Z","iopub.execute_input":"2024-04-25T13:21:05.828085Z","iopub.status.idle":"2024-04-25T13:21:05.834594Z","shell.execute_reply.started":"2024-04-25T13:21:05.828060Z","shell.execute_reply":"2024-04-25T13:21:05.833608Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/summariser-data2022/train.csv')     \n# df_val = pd.read_csv('/kaggle/input/summariser-data2022/val.csv')     ","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:05.836173Z","iopub.execute_input":"2024-04-25T13:21:05.837113Z","iopub.status.idle":"2024-04-25T13:21:06.602578Z","shell.execute_reply.started":"2024-04-25T13:21:05.837070Z","shell.execute_reply":"2024-04-25T13:21:06.601391Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"article_limit = 300\n# summary_limit=\ndef resize(article,summary):\n    a=[]\n    s=[]\n    for i in range(len(article)):\n        if(len(article[i].split())>article_limit):\n            a.append(article[i])\n            s.append(summary[i])\n        else:\n            continue\n    \n    return a,s","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:06.604952Z","iopub.execute_input":"2024-04-25T13:21:06.605260Z","iopub.status.idle":"2024-04-25T13:21:06.612284Z","shell.execute_reply.started":"2024-04-25T13:21:06.605235Z","shell.execute_reply":"2024-04-25T13:21:06.611284Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"train_article = df_train['Article'].tolist()[:5000]\ntrain_summary = df_train['Summary'].tolist()[:5000]\ntrain_article,train_summary = resize(train_article,train_summary)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:06.613676Z","iopub.execute_input":"2024-04-25T13:21:06.614091Z","iopub.status.idle":"2024-04-25T13:21:06.819566Z","shell.execute_reply.started":"2024-04-25T13:21:06.614049Z","shell.execute_reply":"2024-04-25T13:21:06.818499Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"len(train_article)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:06.822302Z","iopub.execute_input":"2024-04-25T13:21:06.822604Z","iopub.status.idle":"2024-04-25T13:21:06.835858Z","shell.execute_reply.started":"2024-04-25T13:21:06.822579Z","shell.execute_reply":"2024-04-25T13:21:06.834887Z"},"trusted":true},"execution_count":63,"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"3097"},"metadata":{}}]},{"cell_type":"code","source":"# from torch.utils.data import Dataset\nfrom datasets import DatasetDict, Dataset\n\nx=100\n\nt=int(len(train_article)*0.9)\nprint(t)\n\ndata_dict = {\n    \"Article\": train_article[:t],\n    \"Summary\": train_summary[:t]\n}\n\n# Create a Dataset object from the dictionary\ndataset_train = Dataset.from_dict(data_dict)\n\ndict_train = DatasetDict({\n    'train': dataset_train,\n    # Add more datasets for validation, test, etc., as needed\n})\n\nprint((dict_train))\n# print(t)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:06.837446Z","iopub.execute_input":"2024-04-25T13:21:06.838035Z","iopub.status.idle":"2024-04-25T13:21:07.089653Z","shell.execute_reply.started":"2024-04-25T13:21:06.837998Z","shell.execute_reply":"2024-04-25T13:21:07.088559Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"2787\nDatasetDict({\n    train: Dataset({\n        features: ['Article', 'Summary'],\n        num_rows: 2787\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# val_article = df_val['Article'].tolist()[:x]\n# val_summary = df_val['Summary'].tolist()[:x]\n\nval_article = train_article[t:]\nval_summary = train_summary[t:] \n\n# val_article,val_summary = resize(val_article,val_summary)\n\ndata_dict = {\n    \"Article\": val_article,\n    \"Summary\": val_summary\n}\n\n# Create a Dataset object from the dictionary\ndataset_val = Dataset.from_dict(data_dict)\n\ndict_val = DatasetDict({\n    'validation': dataset_val,\n    # Add more datasets for validation, test, etc., as needed\n})\n\nprint(type(dict_val))\ndict_val","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:07.091139Z","iopub.execute_input":"2024-04-25T13:21:07.092194Z","iopub.status.idle":"2024-04-25T13:21:07.152304Z","shell.execute_reply.started":"2024-04-25T13:21:07.092153Z","shell.execute_reply":"2024-04-25T13:21:07.151261Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"<class 'datasets.dataset_dict.DatasetDict'>\n","output_type":"stream"},{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    validation: Dataset({\n        features: ['Article', 'Summary'],\n        num_rows: 310\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/IndicBART\")\n# model = AutoModelForSeq2SeqLM.from_pretrained(\"ai4bharat/IndicBART\")\n# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# tokenizer = AutoTokenizer.from_pretrained(\"Someman/bart-hindi\")\n# model = AutoModelForSeq2SeqLM.from_pretrained(\"Someman/bart-hindi\")\n\ntokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/mT5_multilingual_XLSum\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"csebuetnlp/mT5_multilingual_XLSum\")\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:07.153846Z","iopub.execute_input":"2024-04-25T13:21:07.154455Z","iopub.status.idle":"2024-04-25T13:21:10.189656Z","shell.execute_reply.started":"2024-04-25T13:21:07.154416Z","shell.execute_reply":"2024-04-25T13:21:10.183257Z"},"trusted":true},"execution_count":66,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[66], line 11\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSeq2SeqLM\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/IndicBART\")\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# model = AutoModelForSeq2SeqLM.from_pretrained(\"ai4bharat/IndicBART\")\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# tokenizer = AutoTokenizer.from_pretrained(\"Someman/bart-hindi\")\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# model = AutoModelForSeq2SeqLM.from_pretrained(\"Someman/bart-hindi\")\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsebuetnlp/mT5_multilingual_XLSum\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSeq2SeqLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsebuetnlp/mT5_multilingual_XLSum\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:837\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    834\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    835\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    836\u001b[0m         )\n\u001b[0;32m--> 837\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2086\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2083\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2084\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2089\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2090\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2094\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2095\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2097\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2098\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2325\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2325\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   2327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   2328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2329\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2330\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:146\u001b[0m, in \u001b[0;36mT5TokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, eos_token, unk_token, pad_token, extra_ids, additional_special_tokens, add_prefix_space, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m     )\n\u001b[1;32m    144\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_slow\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43munk_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file \u001b[38;5;241m=\u001b[39m vocab_file\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extra_ids \u001b[38;5;241m=\u001b[39m extra_ids\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:114\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m TokenizerFast\u001b[38;5;241m.\u001b[39mfrom_file(fast_tokenizer_file)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# We need to convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_slow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslow_tokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslow_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# We need to create and convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     slow_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslow_tokenizer_class(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:1525\u001b[0m, in \u001b[0;36mconvert_slow_tokenizer\u001b[0;34m(transformer_tokenizer)\u001b[0m\n\u001b[1;32m   1517\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn instance of tokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot be converted in a Fast tokenizer instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m No converter was found. Currently available slow->fast convertors:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1520\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1521\u001b[0m     )\n\u001b[1;32m   1523\u001b[0m converter_class \u001b[38;5;241m=\u001b[39m SLOW_TO_FAST_CONVERTERS[tokenizer_class_name]\n\u001b[0;32m-> 1525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverter_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconverted()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:545\u001b[0m, in \u001b[0;36mSpmConverter.__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    543\u001b[0m m \u001b[38;5;241m=\u001b[39m model_pb2\u001b[38;5;241m.\u001b[39mModelProto()\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moriginal_tokenizer\u001b[38;5;241m.\u001b[39mvocab_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 545\u001b[0m     \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParseFromString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproto \u001b[38;5;241m=\u001b[39m m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproto\u001b[38;5;241m.\u001b[39mtrainer_spec\u001b[38;5;241m.\u001b[39mbyte_fallback:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# from transformers import AutoTokenizer\n# import torch\n\n# df_test = pd.read_csv('/kaggle/input/summariser-data2022/test.csv')\n# text= df_test['Article'].tolist()[12]\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model = model.to(device)\n\n# inputs = tokenizer(text, return_tensors=\"pt\")\n\n# # Move the input tensors to the same device as the model\n# inputs = {k: v.to(device) for k, v in inputs.items()}\n\n# # Generate outputs using the model\n# with torch.no_grad():\n#     # Access the correct key for `input_ids` in the `inputs` dictionary\n#     input_ids = inputs[\"input_ids\"]\n#     outputs = model.generate(input_ids, max_new_tokens=70, do_sample=False)\n\n# generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n# print(\"Generated Text:\", generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.190844Z","iopub.status.idle":"2024-04-25T13:21:10.191367Z","shell.execute_reply.started":"2024-04-25T13:21:10.191099Z","shell.execute_reply":"2024-04-25T13:21:10.191119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# summary = df_test['Summary'].tolist()[12]\n# summary","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.193152Z","iopub.status.idle":"2024-04-25T13:21:10.193588Z","shell.execute_reply.started":"2024-04-25T13:21:10.193366Z","shell.execute_reply":"2024-04-25T13:21:10.193385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# text","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.195187Z","iopub.status.idle":"2024-04-25T13:21:10.195617Z","shell.execute_reply.started":"2024-04-25T13:21:10.195397Z","shell.execute_reply":"2024-04-25T13:21:10.195416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_input_length = 600\nmax_target_length = 40\n\nprefix=\"summarize: \"\nx=10\ndef preprocess_function(examples):\n    inputs = [prefix + doc for doc in examples[\"Article\"]]\n    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True,padding='max_length')\n\n    # Setup the tokenizer for targets\n    labels = tokenizer(text_target=examples[\"Summary\"], max_length=max_target_length, truncation=True,padding='max_length')\n#     labels = tokenizer(text_target=examples[\"Summary\"], padding='longest', truncation=True)\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.196706Z","iopub.status.idle":"2024-04-25T13:21:10.197162Z","shell.execute_reply.started":"2024-04-25T13:21:10.196943Z","shell.execute_reply":"2024-04-25T13:21:10.196961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# inp = preprocess_function(dataset_train['train'])\n\n# x=10\n# tokenised_data_train = dataset_train.map(preprocess_function, batched=True)\n# tokenised_data_val = dataset_val.map(preprocess_function, batched=True)\n\ntokenised_data_train = dict_train.map(preprocess_function, batched=True)\ntokenised_data_val = dict_val.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.198267Z","iopub.status.idle":"2024-04-25T13:21:10.198692Z","shell.execute_reply.started":"2024-04-25T13:21:10.198475Z","shell.execute_reply":"2024-04-25T13:21:10.198494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"(tokenised_data_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.200409Z","iopub.status.idle":"2024-04-25T13:21:10.200905Z","shell.execute_reply.started":"2024-04-25T13:21:10.200631Z","shell.execute_reply":"2024-04-25T13:21:10.200650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nimport numpy as np\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # Rouge expects a newline after each sentence\n    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n    \n    # Note that other metrics may not have a `use_aggregator` parameter\n    # and thus will return a list, computing a metric for each sentence.\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)\n    # Extract a few results\n    result = {key: value * 100 for key, value in result.items()}\n    \n    # Add mean generated length\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    \n    return {k: round(v, 4) for k, v in result.items()}","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.202531Z","iopub.status.idle":"2024-04-25T13:21:10.202900Z","shell.execute_reply.started":"2024-04-25T13:21:10.202708Z","shell.execute_reply":"2024-04-25T13:21:10.202723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model='mT5_multilingual_XLSum')","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.203998Z","iopub.status.idle":"2024-04-25T13:21:10.204323Z","shell.execute_reply.started":"2024-04-25T13:21:10.204163Z","shell.execute_reply":"2024-04-25T13:21:10.204177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install rouge_score\n!pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.205668Z","iopub.status.idle":"2024-04-25T13:21:10.206044Z","shell.execute_reply.started":"2024-04-25T13:21:10.205883Z","shell.execute_reply":"2024-04-25T13:21:10.205898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\nimport evaluate\n\nrouge = evaluate.load(\"rouge\")\n\n\nbatch_size = 1\n# model_name = model_checkpoint.split(\"/\")[-1]\nargs = Seq2SeqTrainingArguments(\n    output_dir=\"mT5_multilingual_XLSum\",\n#     eval_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    weight_decay=0.01,\n    save_total_limit=1,\n    num_train_epochs=1,\n    predict_with_generate=True,\n    fp16=True,\n)\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.208632Z","iopub.status.idle":"2024-04-25T13:21:10.209013Z","shell.execute_reply.started":"2024-04-25T13:21:10.208840Z","shell.execute_reply":"2024-04-25T13:21:10.208856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.210024Z","iopub.status.idle":"2024-04-25T13:21:10.210374Z","shell.execute_reply.started":"2024-04-25T13:21:10.210210Z","shell.execute_reply":"2024-04-25T13:21:10.210224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenised_data_train","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.212372Z","iopub.status.idle":"2024-04-25T13:21:10.212711Z","shell.execute_reply.started":"2024-04-25T13:21:10.212549Z","shell.execute_reply":"2024-04-25T13:21:10.212564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model,\n    args,\n    train_dataset=tokenised_data_train[\"train\"],\n    eval_dataset=tokenised_data_val[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n#      data_collator=data_collator,\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.214366Z","iopub.status.idle":"2024-04-25T13:21:10.214684Z","shell.execute_reply.started":"2024-04-25T13:21:10.214529Z","shell.execute_reply":"2024-04-25T13:21:10.214542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.215706Z","iopub.status.idle":"2024-04-25T13:21:10.216041Z","shell.execute_reply.started":"2024-04-25T13:21:10.215886Z","shell.execute_reply":"2024-04-25T13:21:10.215900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = df_train['Article'].tolist()[11]\ntext='summarise: '+text\ntext","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.217515Z","iopub.status.idle":"2024-04-25T13:21:10.217888Z","shell.execute_reply.started":"2024-04-25T13:21:10.217692Z","shell.execute_reply":"2024-04-25T13:21:10.217706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save tokenizer\nimport pickle\nwith open('/kaggle/working/tokenizer.pkl', 'wb') as f:\n    pickle.dump(tokenizer, f)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.219338Z","iopub.status.idle":"2024-04-25T13:21:10.219791Z","shell.execute_reply.started":"2024-04-25T13:21:10.219544Z","shell.execute_reply":"2024-04-25T13:21:10.219562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\n# Save model\nwith open('mt5_xl_23_translated_model.pkl', 'wb') as f:\n    pickle.dump(model, f)\n\n# Save tokenizer\nwith open('mt5_xl_23_translated_tokenizer.pkl', 'wb') as f:\n    pickle.dump(tokenizer, f)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.221524Z","iopub.status.idle":"2024-04-25T13:21:10.221876Z","shell.execute_reply.started":"2024-04-25T13:21:10.221681Z","shell.execute_reply":"2024-04-25T13:21:10.221701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndf_test = pd.read_csv('/kaggle/input/summariser-data2022/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.222950Z","iopub.status.idle":"2024-04-25T13:21:10.223292Z","shell.execute_reply.started":"2024-04-25T13:21:10.223116Z","shell.execute_reply":"2024-04-25T13:21:10.223130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\n# prin\nwith open('/kaggle/working/Indic_Bart.pkl', 'rb') as f:\n    model = pickle.load(f)\ntext= df_test['Article'].tolist()[12]\nsummarizer = pipeline(\"summarization\", model=model,tokenizer=AutoTokenizer.from_pretrained(\"/kaggle/working/tokeniser\"))\nsummarizer(text)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.224632Z","iopub.status.idle":"2024-04-25T13:21:10.224995Z","shell.execute_reply.started":"2024-04-25T13:21:10.224830Z","shell.execute_reply":"2024-04-25T13:21:10.224844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.226312Z","iopub.status.idle":"2024-04-25T13:21:10.226643Z","shell.execute_reply.started":"2024-04-25T13:21:10.226482Z","shell.execute_reply":"2024-04-25T13:21:10.226496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t=df_test['Summary'].tolist()[12]\nt","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.228052Z","iopub.status.idle":"2024-04-25T13:21:10.228392Z","shell.execute_reply.started":"2024-04-25T13:21:10.228228Z","shell.execute_reply":"2024-04-25T13:21:10.228242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/working/Indic_Bart.pkl', 'rb') as f:\n    model = pickle.load(f)\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/tokeniser\")","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.229659Z","iopub.status.idle":"2024-04-25T13:21:10.230029Z","shell.execute_reply.started":"2024-04-25T13:21:10.229864Z","shell.execute_reply":"2024-04-25T13:21:10.229878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\ninputs = tokenizer(text, return_tensors=\"pt\")\n\n# Move the input tensors to the same device as the model\ninputs = {k: v.to(device) for k, v in inputs.items()}\n\n# Generate outputs using the model\nwith torch.no_grad():\n    # Access the correct key for `input_ids` in the `inputs` dictionary\n    input_ids = inputs[\"input_ids\"]\n    outputs = model.generate(input_ids, max_new_tokens=100, do_sample=False)\n\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(\"Generated Text:\", generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.231163Z","iopub.status.idle":"2024-04-25T13:21:10.231492Z","shell.execute_reply.started":"2024-04-25T13:21:10.231333Z","shell.execute_reply":"2024-04-25T13:21:10.231346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.232417Z","iopub.status.idle":"2024-04-25T13:21:10.232716Z","shell.execute_reply.started":"2024-04-25T13:21:10.232568Z","shell.execute_reply":"2024-04-25T13:21:10.232581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"t=df_test['Summary'].tolist()[12]\nt","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.234940Z","iopub.status.idle":"2024-04-25T13:21:10.235379Z","shell.execute_reply.started":"2024-04-25T13:21:10.235148Z","shell.execute_reply":"2024-04-25T13:21:10.235167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.236438Z","iopub.status.idle":"2024-04-25T13:21:10.236912Z","shell.execute_reply.started":"2024-04-25T13:21:10.236656Z","shell.execute_reply":"2024-04-25T13:21:10.236675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install indic-nlp-library\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.238667Z","iopub.status.idle":"2024-04-25T13:21:10.239007Z","shell.execute_reply.started":"2024-04-25T13:21:10.238851Z","shell.execute_reply":"2024-04-25T13:21:10.238865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\nimport torch\n\n# with open('/kaggle/working/Indic_Bart.pkl', 'rb') as f:\n#     model = pickle.load(f)\n# tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/tokeniser\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# def summarise(s):\n\ninputs = tokenizer(text, return_tensors=\"pt\")\n\n# Move the input tensors to the same device as the model\ninputs = {k: v.to(device) for k, v in inputs.items()}\n\n# Generate outputs using the model\nwith torch.no_grad():\n    # Access the correct key for `input_ids` in the `inputs` dictionary\n    input_ids = inputs[\"input_ids\"]\n    outputs = model.generate(input_ids, max_new_tokens=80, do_sample=False)\n\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n# return generated_text\nprint(\"Generated Text:\", generated_text)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.240052Z","iopub.status.idle":"2024-04-25T13:21:10.240357Z","shell.execute_reply.started":"2024-04-25T13:21:10.240200Z","shell.execute_reply":"2024-04-25T13:21:10.240213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summaries = []\nori_summary = []\ns=df_test['Summary'].tolist()\na = df_test['Article'].tolist()\nprint(len(s))\nori_summary.append(s[0])\nprint(a[0])\n# summaries.append(summarise(a[0]))\n# for i in range(len(s)):\n#     ori_summary.append(s[i])\n#     summaries.append(summarise(a[i]))","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.241457Z","iopub.status.idle":"2024-04-25T13:21:10.241813Z","shell.execute_reply.started":"2024-04-25T13:21:10.241622Z","shell.execute_reply":"2024-04-25T13:21:10.241637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install rouge","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.243279Z","iopub.status.idle":"2024-04-25T13:21:10.243609Z","shell.execute_reply.started":"2024-04-25T13:21:10.243449Z","shell.execute_reply":"2024-04-25T13:21:10.243463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test2=df_train[:100]\ndf_test2","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.245332Z","iopub.status.idle":"2024-04-25T13:21:10.245668Z","shell.execute_reply.started":"2024-04-25T13:21:10.245509Z","shell.execute_reply":"2024-04-25T13:21:10.245523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef predict_summary(text):\n    inputs = tokenizer(text, return_tensors='pt', max_length=600, truncation=True)\n    inputs.to(\"cuda\")\n    summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=40, early_stopping=True)\n    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    return summary\n\ndf_test2['predicted_summary'] = df_test2['Article'].apply(predict_summary)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.246712Z","iopub.status.idle":"2024-04-25T13:21:10.247164Z","shell.execute_reply.started":"2024-04-25T13:21:10.246935Z","shell.execute_reply":"2024-04-25T13:21:10.246954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from rouge import Rouge\n\nrouge = Rouge()\n\n\ndef calculate_rouge(row):\n    return rouge.get_scores(row['predicted_summary'], row['Summary'], avg=True)\n\ndf_test2['rouge'] = df_test2.apply(calculate_rouge, axis=1)\ndf_test2['rouge-1'] = df_test2['rouge'].apply(lambda x: x['rouge-1']['f'])\ndf_test2['rouge-2'] = df_test2['rouge'].apply(lambda x: x['rouge-2']['f'])\n\nprint(df_test2['rouge-1'].mean())\nprint(df_test2['rouge-2'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.252034Z","iopub.status.idle":"2024-04-25T13:21:10.252383Z","shell.execute_reply.started":"2024-04-25T13:21:10.252210Z","shell.execute_reply":"2024-04-25T13:21:10.252225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install rouge","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.253579Z","iopub.status.idle":"2024-04-25T13:21:10.253917Z","shell.execute_reply.started":"2024-04-25T13:21:10.253748Z","shell.execute_reply":"2024-04-25T13:21:10.253768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Define compute_metrics function\n# def compute_metrics(eval_pred):\n#     predictions, labels = eval_pred\n#     decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n#     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n#     # Load ROUGE and BERT score metrics\n#     rouge = load_metric('rouge')\n#     bert_score = load_metric('bertscore')\n\n#     # Compute ROUGE and BERT scores\n#     rouge_output = rouge.compute(predictions=decoded_preds, references=decoded_labels, rouge_types=[\"rougeL\", \"rouge2\"])\n#     bert_score_output = bert_score.compute(predictions=decoded_preds, references=decoded_labels, lang=\"hi\")\n\n#     return {\n#         \"rouge2\": rouge_output['rouge2'].mid.fmeasure,\n#         \"rougeL\": rouge_output['rougeL'].mid.fmeasure,\n#         \"bert_score\": bert_score_output['f1'][0]\n#     }","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:21:10.255178Z","iopub.status.idle":"2024-04-25T13:21:10.255615Z","shell.execute_reply.started":"2024-04-25T13:21:10.255387Z","shell.execute_reply":"2024-04-25T13:21:10.255406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"********************************************\n\n**Testing Phase**\n\n***********************************************","metadata":{}},{"cell_type":"code","source":"!pip install rouge_score\n!pip install rouge","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:28:55.538726Z","iopub.execute_input":"2024-04-25T13:28:55.539099Z","iopub.status.idle":"2024-04-25T13:29:25.920968Z","shell.execute_reply.started":"2024-04-25T13:28:55.539068Z","shell.execute_reply":"2024-04-25T13:29:25.919796Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=32f8930671282f37d01c18b5d5c137b46ee9d17c5fbbb74c73c26d10db9779df\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\nCollecting rouge\n  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge) (1.16.0)\nDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\nInstalling collected packages: rouge\nSuccessfully installed rouge-1.0.1\n","output_type":"stream"}]},{"cell_type":"code","source":"import pickle\nimport torch\nimport pandas as pd\n\n\nwith open('/kaggle/working/mt5_xl_23_translated_model.pkl', 'rb') as f:\n    model = pickle.load(f)\n    \nwith open('/kaggle/working/mt5_xl_23_translated_tokenizer.pkl', 'rb') as f:\n    tokenizer = pickle.load(f)\n    \n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n# model.to(tokenizer)\n\ndef file_read(path):\n    return pd.read_csv(path)\n\ndf_val = file_read('/kaggle/input/summariser-data2022/train.csv')\ndf_test = file_read('/kaggle/input/summariser-data2022/test.csv')\n\ndf_test = df_test.sample(frac=0.2)\n\n\n\nfrom rouge import Rouge\ndef predict_summary(text):\n    inputs = tokenizer(text, return_tensors='pt', max_length=600, truncation=True)\n    inputs.to(\"cuda\")\n    summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=40, early_stopping=True)\n    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    return summary\n\ndef score(df):\n\n    df['predicted_summary'] = df['Article'].apply(predict_summary)\n\n    rouge = Rouge()\n\n\n    def calculate_rouge(row):\n        return rouge.get_scores(row['predicted_summary'], row['Summary'], avg=True)\n\n    df['rouge'] = df_test.apply(calculate_rouge, axis=1)\n    df['rouge-1'] = df_test['rouge'].apply(lambda x: x['rouge-1']['f'])\n    df['rouge-2'] = df_test['rouge'].apply(lambda x: x['rouge-2']['f'])\n\n    print(df['rouge-1'].mean())\n    print(df['rouge-2'].mean())\n\n    df_test['rouge-l'] = df_test['rouge'].apply(lambda x: x['rouge-l']['f'])\n    print(df_test['rouge-l'].mean())\n\nscore(df_test)\n\nscore(df_val)\n\nprint(\"Rouge-1 score :\" ,df_test['rouge-1'].mean())\nprint(\"Rouge-2 score :\", df_test['rouge-2'].mean())\nprint(\"Rouge-L score :\", df_test['rouge-l'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-04-25T13:29:53.993309Z","iopub.execute_input":"2024-04-25T13:29:53.994059Z","iopub.status.idle":"2024-04-25T13:30:00.610515Z","shell.execute_reply.started":"2024-04-25T13:29:53.994025Z","shell.execute_reply":"2024-04-25T13:30:00.609165Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/mt5_xl_23_translated_model.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      7\u001b[0m     model \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/working/mt5_xl_23_translated_tokenizer.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/mt5_xl_23_translated_model.pkl'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/working/mt5_xl_23_translated_model.pkl'","output_type":"error"}]}]}