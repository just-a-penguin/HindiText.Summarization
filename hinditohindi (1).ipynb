{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8184236,"sourceType":"datasetVersion","datasetId":4845936}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import csv\nimport pandas as pd \nimport torch\nimport torch.nn as nn\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:48:54.693446Z","iopub.execute_input":"2024-04-22T20:48:54.694136Z","iopub.status.idle":"2024-04-22T20:48:59.060968Z","shell.execute_reply.started":"2024-04-22T20:48:54.694103Z","shell.execute_reply":"2024-04-22T20:48:59.060133Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\n# s='train[:10]'\ndataset_train = load_dataset('csv', data_files='/kaggle/input/summariser-data2022/train.csv')\ndataset_val = load_dataset('csv', data_files='/kaggle/input/summariser-data2022/val.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:48:59.062575Z","iopub.execute_input":"2024-04-22T20:48:59.062971Z","iopub.status.idle":"2024-04-22T20:49:02.652779Z","shell.execute_reply.started":"2024-04-22T20:48:59.062945Z","shell.execute_reply":"2024-04-22T20:49:02.651782Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a48f17a983764c28917fd09695dc2414"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a4af4250ac04284a38f0e7b5a32c4bd"}},"metadata":{}}]},{"cell_type":"code","source":"dataset_train","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:49:02.654255Z","iopub.execute_input":"2024-04-22T20:49:02.654700Z","iopub.status.idle":"2024-04-22T20:49:02.662187Z","shell.execute_reply.started":"2024-04-22T20:49:02.654674Z","shell.execute_reply":"2024-04-22T20:49:02.661332Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'Article', 'Heading', 'Summary'],\n        num_rows: 7957\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"dataset_val","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:49:02.664277Z","iopub.execute_input":"2024-04-22T20:49:02.664558Z","iopub.status.idle":"2024-04-22T20:49:02.674172Z","shell.execute_reply.started":"2024-04-22T20:49:02.664535Z","shell.execute_reply":"2024-04-22T20:49:02.673207Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'Article', 'Heading', 'Summary'],\n        num_rows: 569\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/summariser-data2022/train.csv')     \ndf_val = pd.read_csv('/kaggle/input/summariser-data2022/val.csv')     ","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:49:02.675370Z","iopub.execute_input":"2024-04-22T20:49:02.675707Z","iopub.status.idle":"2024-04-22T20:49:03.460943Z","shell.execute_reply.started":"2024-04-22T20:49:02.675672Z","shell.execute_reply":"2024-04-22T20:49:03.460139Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# from torch.utils.data import Dataset\nfrom datasets import DatasetDict, Dataset\n\nx=None\ntrain_article = df_train['Article'].tolist()[:x]\ntrain_summary = df_train['Summary'].tolist()[:x]\n\n\n\ndata_dict = {\n    \"Article\": train_article,\n    \"Summary\": train_summary\n}\n\n# Create a Dataset object from the dictionary\ndataset_train = Dataset.from_dict(data_dict)\n\ndict_train = DatasetDict({\n    'train': dataset_train,\n    # Add more datasets for validation, test, etc., as needed\n})\n\nprint((dict_train))\n# print(t)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:49:03.462123Z","iopub.execute_input":"2024-04-22T20:49:03.462416Z","iopub.status.idle":"2024-04-22T20:49:03.958234Z","shell.execute_reply.started":"2024-04-22T20:49:03.462391Z","shell.execute_reply":"2024-04-22T20:49:03.957318Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['Article', 'Summary'],\n        num_rows: 7957\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"\nval_article = df_val['Article'].tolist()[:x]\nval_summary = df_val['Summary'].tolist()[:x]\n\ndata_dict = {\n    \"Article\": train_article,\n    \"Summary\": train_summary\n}\n\n# Create a Dataset object from the dictionary\ndataset_train = Dataset.from_dict(data_dict)\n\ndict_val = DatasetDict({\n    'train': dataset_train,\n    # Add more datasets for validation, test, etc., as needed\n})\n\nprint(type(dict_val))","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:49:03.959620Z","iopub.execute_input":"2024-04-22T20:49:03.960001Z","iopub.status.idle":"2024-04-22T20:49:04.349042Z","shell.execute_reply.started":"2024-04-22T20:49:03.959968Z","shell.execute_reply":"2024-04-22T20:49:04.348158Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"<class 'datasets.dataset_dict.DatasetDict'>\n","output_type":"stream"}]},{"cell_type":"code","source":"# from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n\n# model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50\")\n# tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50\", src_lang=\"hi_IN\", tgt_lang=\"hi_IN\")\n\n# viditraj860/mbart-finetuned-hindi-3012\n# model = MBartForConditionalGeneration.from_pretrained(\"viditraj860/mbart-finetuned-hindi-3012\",from_tf=True)\n# tokenizer = MBart50TokenizerFast.from_pretrained(\"viditraj860/mbart-finetuned-hindi-3012\")\n\n# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google/mt5-base\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-base\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:49:04.350369Z","iopub.execute_input":"2024-04-22T20:49:04.350659Z","iopub.status.idle":"2024-04-22T20:49:37.151688Z","shell.execute_reply.started":"2024-04-22T20:49:04.350634Z","shell.execute_reply":"2024-04-22T20:49:37.150729Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"2024-04-22 20:49:09.449119: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-22 20:49:09.449234: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-22 20:49:09.585019: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/376 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"182663eaee5949539df1c0a1ba5ee011"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/702 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd1abf84c3e142568e9c67148c758565"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"add2fff0e09e4f208b58c3326840d147"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b87411765a4846039bb2b1669fab7ff1"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.33G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c0b72e7bd6447efaec936cc127eefcb"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a17ac0383b8e4381bbd48ca56af65e7c"}},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"MT5ForConditionalGeneration(\n  (shared): Embedding(250112, 768)\n  (encoder): MT5Stack(\n    (embed_tokens): Embedding(250112, 768)\n    (block): ModuleList(\n      (0): MT5Block(\n        (layer): ModuleList(\n          (0): MT5LayerSelfAttention(\n            (SelfAttention): MT5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): MT5LayerFF(\n            (DenseReluDense): MT5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x MT5Block(\n        (layer): ModuleList(\n          (0): MT5LayerSelfAttention(\n            (SelfAttention): MT5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): MT5LayerFF(\n            (DenseReluDense): MT5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): MT5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): MT5Stack(\n    (embed_tokens): Embedding(250112, 768)\n    (block): ModuleList(\n      (0): MT5Block(\n        (layer): ModuleList(\n          (0): MT5LayerSelfAttention(\n            (SelfAttention): MT5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n              (relative_attention_bias): Embedding(32, 12)\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): MT5LayerCrossAttention(\n            (EncDecAttention): MT5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): MT5LayerFF(\n            (DenseReluDense): MT5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-11): 11 x MT5Block(\n        (layer): ModuleList(\n          (0): MT5LayerSelfAttention(\n            (SelfAttention): MT5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): MT5LayerCrossAttention(\n            (EncDecAttention): MT5Attention(\n              (q): Linear(in_features=768, out_features=768, bias=False)\n              (k): Linear(in_features=768, out_features=768, bias=False)\n              (v): Linear(in_features=768, out_features=768, bias=False)\n              (o): Linear(in_features=768, out_features=768, bias=False)\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): MT5LayerFF(\n            (DenseReluDense): MT5DenseGatedActDense(\n              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=768, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): MT5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): MT5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=768, out_features=250112, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"max_input_length = 1024\nmax_target_length = 100\n\nprefix=\"summarize: \"\nx=10\ndef preprocess_function(examples):\n    inputs = [prefix + doc for doc in examples[\"Article\"]]\n    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n\n    # Setup the tokenizer for targets\n    labels = tokenizer(text_target=examples[\"Summary\"], max_length=max_target_length, truncation=True)\n#     labels = tokenizer(text_target=examples[\"Summary\"], padding='longest', truncation=True)\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:49:37.153076Z","iopub.execute_input":"2024-04-22T20:49:37.153798Z","iopub.status.idle":"2024-04-22T20:49:37.160485Z","shell.execute_reply.started":"2024-04-22T20:49:37.153763Z","shell.execute_reply":"2024-04-22T20:49:37.159585Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# inp = preprocess_function(dataset_train['train'])\n\n# x=10\n# tokenised_data_train = dataset_train.map(preprocess_function, batched=True)\n# tokenised_data_val = dataset_val.map(preprocess_function, batched=True)\n\ntokenised_data_train = dict_train.map(preprocess_function, batched=True)\ntokenised_data_val = dict_val.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:49:37.164011Z","iopub.execute_input":"2024-04-22T20:49:37.164304Z","iopub.status.idle":"2024-04-22T20:50:06.372636Z","shell.execute_reply.started":"2024-04-22T20:49:37.164278Z","shell.execute_reply":"2024-04-22T20:50:06.371764Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7957 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd3cadb917f0459cb58522ba989e1b84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/7957 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dec5ce5e6b8346ff8224a2158e6c6f5c"}},"metadata":{}}]},{"cell_type":"code","source":"type(tokenised_data_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:50:06.373776Z","iopub.execute_input":"2024-04-22T20:50:06.374098Z","iopub.status.idle":"2024-04-22T20:50:06.380266Z","shell.execute_reply.started":"2024-04-22T20:50:06.374071Z","shell.execute_reply":"2024-04-22T20:50:06.379338Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"datasets.dataset_dict.DatasetDict"},"metadata":{}}]},{"cell_type":"code","source":"import nltk\nimport numpy as np\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # Rouge expects a newline after each sentence\n    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n    \n    # Note that other metrics may not have a `use_aggregator` parameter\n    # and thus will return a list, computing a metric for each sentence.\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)\n    # Extract a few results\n    result = {key: value * 100 for key, value in result.items()}\n    \n    # Add mean generated length\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    \n    return {k: round(v, 4) for k, v in result.items()}","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:50:06.381759Z","iopub.execute_input":"2024-04-22T20:50:06.382047Z","iopub.status.idle":"2024-04-22T20:50:06.839446Z","shell.execute_reply.started":"2024-04-22T20:50:06.382023Z","shell.execute_reply":"2024-04-22T20:50:06.838331Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model='google/mt5-base')","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:50:06.842110Z","iopub.execute_input":"2024-04-22T20:50:06.842732Z","iopub.status.idle":"2024-04-22T20:50:06.848383Z","shell.execute_reply.started":"2024-04-22T20:50:06.842699Z","shell.execute_reply":"2024-04-22T20:50:06.846679Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"!pip install rouge_score\n!pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:50:06.849993Z","iopub.execute_input":"2024-04-22T20:50:06.850401Z","iopub.status.idle":"2024-04-22T20:50:35.510081Z","shell.execute_reply.started":"2024-04-22T20:50:06.850369Z","shell.execute_reply":"2024-04-22T20:50:35.508850Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=c9ae925409f8438d59a70ce5be6c4714aa697bd8aa13102776001502df410091\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.18.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.2.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.22.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nCollecting responses<0.19 (from evaluate)\n  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading responses-0.18.0-py3-none-any.whl (38 kB)\nInstalling collected packages: responses, evaluate\nSuccessfully installed evaluate-0.4.1 responses-0.18.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\nimport evaluate\n\nrouge = evaluate.load(\"rouge\")\n\n\nbatch_size = 4\n# model_name = model_checkpoint.split(\"/\")[-1]\nargs = Seq2SeqTrainingArguments(\n    output_dir=\"mt5\",\n#     eval_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    weight_decay=0.01,\n    save_total_limit=3,\n    num_train_epochs=1,\n    predict_with_generate=True,\n    fp16=True,\n)\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:51:56.164884Z","iopub.execute_input":"2024-04-22T20:51:56.165776Z","iopub.status.idle":"2024-04-22T20:51:56.864882Z","shell.execute_reply.started":"2024-04-22T20:51:56.165743Z","shell.execute_reply":"2024-04-22T20:51:56.863630Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:51:59.432690Z","iopub.execute_input":"2024-04-22T20:51:59.433428Z","iopub.status.idle":"2024-04-22T20:51:59.439734Z","shell.execute_reply.started":"2024-04-22T20:51:59.433394Z","shell.execute_reply":"2024-04-22T20:51:59.438574Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"tokenised_data_train","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:50:36.426565Z","iopub.execute_input":"2024-04-22T20:50:36.427182Z","iopub.status.idle":"2024-04-22T20:50:36.442313Z","shell.execute_reply.started":"2024-04-22T20:50:36.427149Z","shell.execute_reply":"2024-04-22T20:50:36.441246Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['Article', 'Summary', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 7957\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model,\n    args,\n    train_dataset=tokenised_data_train[\"train\"],\n    eval_dataset=tokenised_data_val[\"train\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n#      data_collator=data_collator,\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:52:01.779214Z","iopub.execute_input":"2024-04-22T20:52:01.779604Z","iopub.status.idle":"2024-04-22T21:20:44.190212Z","shell.execute_reply.started":"2024-04-22T20:52:01.779576Z","shell.execute_reply":"2024-04-22T21:20:44.189270Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1990' max='1990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1990/1990 28:40, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1990, training_loss=0.0, metrics={'train_runtime': 1721.8182, 'train_samples_per_second': 4.621, 'train_steps_per_second': 1.156, 'total_flos': 1.781534311704576e+16, 'train_loss': 0.0, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"text = df_train['Article'].tolist()[11]\ntext","metadata":{"execution":{"iopub.status.busy":"2024-04-22T21:21:25.534842Z","iopub.execute_input":"2024-04-22T21:21:25.535246Z","iopub.status.idle":"2024-04-22T21:21:25.553853Z","shell.execute_reply.started":"2024-04-22T21:21:25.535190Z","shell.execute_reply":"2024-04-22T21:21:25.552951Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"\"नई दिल्ली: पूर्व राष्ट्रपति प्रणब मुखर्जी की हालत लगातार नाजुक बनी हुई है और वह जीवनरक्षक प्रणाली पर हैं। सेना के रिसर्च एंड रेफरल अस्पताल ने बुधवार को एक बयान में कहा, 'प्रणब मुखर्जी की हालत लगातार नाजुक बनी हुई है। इस समय उनकी हालत रक्त प्रवाह के लिहाज से स्थिर है और वह वेंटिलेटर पर हैं।' मुखर्जी (84) को सोमवार को यहां सैन्य अस्पताल में भर्ती कराया गया था और मस्तिष्क की सर्जरी से पहले उनके कोरोना वायरस से संक्रमित होने की पुष्टि हुई थी। उन्हें देख रहे डॉक्टरों ने कहा कि मंगलवार को पूर्व राष्ट्रपति की हालत बिगड़ गई और उनकी स्थिति में सुधार का कोई लक्षण नहीं दिखा है।प्रणब मुखर्जी के बेटे अभिजीत मुखर्जी ने ट्वीट कर जानकारी दी है कि 'उनकी तबीयत हेमोडाइनेमिकली स्थिर है। मैं सभी से अनुरोध करता हूं कि वह आपकी प्रार्थनाओं को जारी रखें और उनकी शीघ्र स्वस्थ होने के लिए शुभकामनाएं। धन्यवाद'पूर्व राष्ट्रपति प्रणब मुखर्जी की बेटी शर्मिष्ठा मुखर्जी ने अपने पिता के अच्छे स्वास्थ्य की कामना करते हुए कहा कि ईश्वर उनके लिए सबकुछ अच्छा करेगा। कांग्रेस नेता शर्मिष्ठा मुखर्जी ने कहा कि देश का सबसे बड़ा नागरिक सम्मान ‘भारत रत्न’ मिलने के मात्र एक साल बाद उनके पिता गंभीर रूप से बीमार हो गए। उन्होंने ट्वीट किया, ‘‘पिछले साल आठ अगस्त का दिन मेरे लिए सबसे खुशी के दिनों में से एक था, क्योंकि मेरे पिता को भारत रत्न मिला था। ठीक एक साल बाद 10 अगस्त को वह गंभीर रूप से बीमार हो गए।’’शर्मिष्ठा ने कहा, ‘‘ईश्वर उनके लिए सबकुछ अच्छा करे और मुझे जीवन की खुशियों एवं दुखों को समान भाव से स्वीकार करने की ताकत प्रदान करे। मैं मेरे पिता के लिए चिंता करने वाले सभी लोगों का धन्यवाद करती हूं।’’सेना के रिसर्च एंड रेफरल (आर एंड आर) अस्पताल ने मंगलवार को बताया था कि प्रणब मुखर्जी की हालत नाजुक बनी हुई है और उन्हें जीवनरक्षक प्रणाली पर रखा गया है। इससे एक दिन पहले उनके मस्तिष्क की सर्जरी की गई थी। मुखर्जी (84) को सोमवार दोपहर के वक्त सैन्य अस्पताल में भर्ती कराया गया था और सर्जरी से पहले उनमें कोविड-19 की भी पुष्टि हुई थी।अस्पताल की ओर से मंगलवार को जारी नए मेडिकल बुलेटिन में कहा गया था, ‘‘पूर्व राष्ट्रपति प्रणब मुखर्जी की हालत नाजुक बनी हुई है और उन्हें जीवनरक्षक प्रणाली पर रखा गया है। खून का थक्का बनने के कारण सोमवार को पूर्व राष्ट्रपति के मस्तिष्क की सर्जरी की गई थी। उनकी हालत में कोई सुधार नजर नहीं आया है और स्थिति नाजुक बनी हुई है।’’ मुखर्जी जुलाई 2012 से 2017 तक देश के राष्ट्रपति रहे।\""},"metadata":{}}]},{"cell_type":"code","source":"import pickle\nwith open('m5.pkl', 'wb') as f:\n    pickle.dump(model, f)\n\n\n\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T21:21:29.450813Z","iopub.execute_input":"2024-04-22T21:21:29.451522Z","iopub.status.idle":"2024-04-22T21:21:34.474992Z","shell.execute_reply.started":"2024-04-22T21:21:29.451491Z","shell.execute_reply":"2024-04-22T21:21:34.473807Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"tokenizer.save_pretrained('tokeniser')","metadata":{"execution":{"iopub.status.busy":"2024-04-22T21:21:38.770573Z","iopub.execute_input":"2024-04-22T21:21:38.771151Z","iopub.status.idle":"2024-04-22T21:21:38.833030Z","shell.execute_reply.started":"2024-04-22T21:21:38.771118Z","shell.execute_reply":"2024-04-22T21:21:38.831988Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"('tokeniser/tokenizer_config.json',\n 'tokeniser/special_tokens_map.json',\n 'tokeniser/spiece.model',\n 'tokeniser/added_tokens.json',\n 'tokeniser/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\nimport torch\n\n# Assuming `model` and `tokenizer` are already defined\n# Move the model to the desired device (e.g., GPU)\n\n\n# Example input text\n# text = \"This is a sample input for generation.\"\n\n# Tokenize the input text\n#load the model file using pickle\nwith open('/kaggle/working/m5.pkl', 'rb') as f:\n    model = pickle.load(f)\n    \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/tokeniser\")\ninputs = tokenizer(text, return_tensors=\"pt\")\n\n# Move the input tensors to the same device as the model\ninputs = {k: v.to(device) for k, v in inputs.items()}\n\n# Generate outputs using the model\nwith torch.no_grad():\n    # Access the correct key for `input_ids` in the `inputs` dictionary\n    input_ids = inputs[\"input_ids\"]\n    outputs = model.generate(input_ids, max_new_tokens=100, do_sample=False)\n\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(\"Generated Text:\", generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T21:21:41.635087Z","iopub.execute_input":"2024-04-22T21:21:41.636354Z","iopub.status.idle":"2024-04-22T21:21:49.292177Z","shell.execute_reply.started":"2024-04-22T21:21:41.636308Z","shell.execute_reply":"2024-04-22T21:21:49.291066Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Generated Text: <extra_id_0> उन्का ।\n","output_type":"stream"}]},{"cell_type":"code","source":"summary = df_train.values.tolist()[11][2]\nprint(summary)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T21:21:53.812214Z","iopub.execute_input":"2024-04-22T21:21:53.812637Z","iopub.status.idle":"2024-04-22T21:21:53.834096Z","shell.execute_reply.started":"2024-04-22T21:21:53.812601Z","shell.execute_reply":"2024-04-22T21:21:53.833217Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"वेंटिलेटर पर प्रणब मुखर्जी, बेटी शर्मिष्ठा ने याद किया 8 अगस्त का वो दिन\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\ninputs = tokenizer(text, return_tensors=\"pt\").input_ids\noutputs = model.generate(inputs, max_new_tokens=100, do_sample=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:51:05.574954Z","iopub.status.idle":"2024-04-22T20:51:05.575405Z","shell.execute_reply.started":"2024-04-22T20:51:05.575160Z","shell.execute_reply":"2024-04-22T20:51:05.575179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import  gc\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:51:05.577238Z","iopub.status.idle":"2024-04-22T20:51:05.577668Z","shell.execute_reply.started":"2024-04-22T20:51:05.577445Z","shell.execute_reply":"2024-04-22T20:51:05.577463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_article = df_train['Article'].tolist()[:10]\ntrain_summary = df_train['Summary'].tolist()[:10]\n\nval_article = df_val['Article'].tolist()\nval_summary = df_val['Summary'].tolist()\n\n\nprint(len(train_article))\nprint(len(train_summary))\nmax_len = 0\nfor i in train_article:\n    max_len = max(max_len, len(i.split()))\nprint(max_len)\n# print(len(val_article))","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:51:05.579361Z","iopub.status.idle":"2024-04-22T20:51:05.579936Z","shell.execute_reply.started":"2024-04-22T20:51:05.579684Z","shell.execute_reply":"2024-04-22T20:51:05.579704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_summary[0].split(\" \"))","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:51:05.581021Z","iopub.status.idle":"2024-04-22T20:51:05.581471Z","shell.execute_reply.started":"2024-04-22T20:51:05.581247Z","shell.execute_reply":"2024-04-22T20:51:05.581266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import EvalPrediction\nfrom transformers import (\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    DataCollatorForSeq2Seq,\n)\n\ndef compute_metrics(p: EvalPrediction):\n    preds = p.predictions[0] if isinstance(p.predictions, \n            tuple) else p.predictions\n    result = multi_label_metrics(\n        predictions=preds, \n        labels=p.label_ids)\n    return result\n\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./mbart\",\n    per_device_train_batch_size=1,\n    num_train_epochs=2,\n    logging_dir=\"./logs\",\n)\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=inputs,\n    compute_metrics=compute_metrics\n#     data_collator=custom_collate_fn\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:51:05.582799Z","iopub.status.idle":"2024-04-22T20:51:05.583254Z","shell.execute_reply.started":"2024-04-22T20:51:05.583013Z","shell.execute_reply":"2024-04-22T20:51:05.583031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n# train_dataset = torch.utils.data.TensorDataset(train_encodings[\"input_ids\"], train_encodings[\"attention_mask\"])\n# val_dataset = torch.utils.data.TensorDataset(val_encodings[\"input_ids\"], val_encodings[\"attention_mask\"])","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:51:05.584711Z","iopub.status.idle":"2024-04-22T20:51:05.585170Z","shell.execute_reply.started":"2024-04-22T20:51:05.584944Z","shell.execute_reply":"2024-04-22T20:51:05.584963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# type(train_encodings)\n# print(train_encodings.keys())","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:51:05.587486Z","iopub.status.idle":"2024-04-22T20:51:05.587926Z","shell.execute_reply.started":"2024-04-22T20:51:05.587696Z","shell.execute_reply":"2024-04-22T20:51:05.587714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import TrainingArguments\n\n# training_args = TrainingArguments(output_dir=\"test_trainer\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:51:05.589236Z","iopub.status.idle":"2024-04-22T20:51:05.589662Z","shell.execute_reply.started":"2024-04-22T20:51:05.589442Z","shell.execute_reply":"2024-04-22T20:51:05.589460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport evaluate\n\nmetric = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:51:05.590858Z","iopub.status.idle":"2024-04-22T20:51:05.591313Z","shell.execute_reply.started":"2024-04-22T20:51:05.591074Z","shell.execute_reply":"2024-04-22T20:51:05.591093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\",num_train_epochs=1,no_cuda=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:51:05.592926Z","iopub.status.idle":"2024-04-22T20:51:05.593404Z","shell.execute_reply.started":"2024-04-22T20:51:05.593137Z","shell.execute_reply":"2024-04-22T20:51:05.593157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import torch\n\n# # Clear CUDA cache\n\n# import gc\n# del variables\n# gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:51:05.594484Z","iopub.status.idle":"2024-04-22T20:51:05.594944Z","shell.execute_reply.started":"2024-04-22T20:51:05.594691Z","shell.execute_reply":"2024-04-22T20:51:05.594710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import TensorDataset\n\n# train_dataset = TensorDataset(*input_tensors)\n# val_dataset = TensorDataset(*val_tensors)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=prepared_dataset,\n    eval_dataset=prepared_dataset_val,\n    compute_metrics=compute_metrics,\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:51:05.596595Z","iopub.status.idle":"2024-04-22T20:51:05.597090Z","shell.execute_reply.started":"2024-04-22T20:51:05.596802Z","shell.execute_reply":"2024-04-22T20:51:05.596820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch, gc\ngc.collect()\ntorch.cuda.empty_cache()\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:51:05.598331Z","iopub.status.idle":"2024-04-22T20:51:05.598754Z","shell.execute_reply.started":"2024-04-22T20:51:05.598534Z","shell.execute_reply":"2024-04-22T20:51:05.598552Z"},"trusted":true},"execution_count":null,"outputs":[]}]}