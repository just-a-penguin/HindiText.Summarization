{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8184236,"sourceType":"datasetVersion","datasetId":4845936},{"sourceId":8204708,"sourceType":"datasetVersion","datasetId":4861189},{"sourceId":8215303,"sourceType":"datasetVersion","datasetId":4869275},{"sourceId":8220822,"sourceType":"datasetVersion","datasetId":4873602}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import csv\nimport pandas as pd \nimport torch\nimport torch.nn as nn\n","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:02:49.495448Z","iopub.execute_input":"2024-04-24T23:02:49.495799Z","iopub.status.idle":"2024-04-24T23:02:54.133551Z","shell.execute_reply.started":"2024-04-24T23:02:49.495770Z","shell.execute_reply":"2024-04-24T23:02:54.132570Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\n# s='train[:10]'\ndataset_train = load_dataset('csv', data_files='/kaggle/input/translated-data/Translated_hindi_train.csv')\n# dataset_val = load_dataset('csv', data_files='/kaggle/input/summariser-data2022/val.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:02:54.135696Z","iopub.execute_input":"2024-04-24T23:02:54.136161Z","iopub.status.idle":"2024-04-24T23:03:00.702702Z","shell.execute_reply.started":"2024-04-24T23:02:54.136133Z","shell.execute_reply":"2024-04-24T23:03:00.701623Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"649d6e1e45644e429c63932dcafb26ce"}},"metadata":{}}]},{"cell_type":"code","source":"dataset_train","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:03:00.704069Z","iopub.execute_input":"2024-04-24T23:03:00.704601Z","iopub.status.idle":"2024-04-24T23:03:00.713319Z","shell.execute_reply.started":"2024-04-24T23:03:00.704571Z","shell.execute_reply":"2024-04-24T23:03:00.712352Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['Id', 'Heading', 'Summary', 'Article'],\n        num_rows: 21225\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/2023-data/hindi_train.csv')     \n# df_val = pd.read_csv('/kaggle/input/summariser-data2022/val.csv')     ","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:03:00.715543Z","iopub.execute_input":"2024-04-24T23:03:00.715867Z","iopub.status.idle":"2024-04-24T23:03:04.626815Z","shell.execute_reply.started":"2024-04-24T23:03:00.715839Z","shell.execute_reply":"2024-04-24T23:03:04.625663Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"article_limit = 300\n# summary_limit=\ndef resize(article,summary):\n    a=[]\n    s=[]\n    for i in range(len(article)):\n        if(len(article[i].split())>article_limit):\n            a.append(article[i])\n            s.append(summary[i])\n        else:\n            continue\n    \n    return a,s","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:03:04.628213Z","iopub.execute_input":"2024-04-24T23:03:04.628613Z","iopub.status.idle":"2024-04-24T23:03:04.635154Z","shell.execute_reply.started":"2024-04-24T23:03:04.628577Z","shell.execute_reply":"2024-04-24T23:03:04.634222Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_article = df_train['Article'].tolist()\ntrain_summary = df_train['Summary'].tolist()\n# train_article,train_summary = resize(train_article,train_summary)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:03:04.636547Z","iopub.execute_input":"2024-04-24T23:03:04.636978Z","iopub.status.idle":"2024-04-24T23:03:04.646467Z","shell.execute_reply.started":"2024-04-24T23:03:04.636947Z","shell.execute_reply":"2024-04-24T23:03:04.645431Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"len(train_article)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:03:04.647746Z","iopub.execute_input":"2024-04-24T23:03:04.648102Z","iopub.status.idle":"2024-04-24T23:03:04.657158Z","shell.execute_reply.started":"2024-04-24T23:03:04.648068Z","shell.execute_reply":"2024-04-24T23:03:04.655959Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"21225"},"metadata":{}}]},{"cell_type":"code","source":"# from torch.utils.data import Dataset\nfrom datasets import DatasetDict, Dataset\n\nx=None\n\nt=int(len(train_article)*0.9)\nprint(t)\n\ndata_dict = {\n    \"Article\": train_article[:t],\n    \"Summary\": train_summary[:t]\n}\n\n# Create a Dataset object from the dictionary\ndataset_train = Dataset.from_dict(data_dict)\n\ndict_train = DatasetDict({\n    'train': dataset_train,\n    # Add more datasets for validation, test, etc., as needed\n})\n\nprint((dict_train))\n# print(t)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:03:04.658514Z","iopub.execute_input":"2024-04-24T23:03:04.658820Z","iopub.status.idle":"2024-04-24T23:03:06.345369Z","shell.execute_reply.started":"2024-04-24T23:03:04.658792Z","shell.execute_reply":"2024-04-24T23:03:06.344360Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"19102\nDatasetDict({\n    train: Dataset({\n        features: ['Article', 'Summary'],\n        num_rows: 19102\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# val_article = df_val['Article'].tolist()[:x]\n# val_summary = df_val['Summary'].tolist()[:x]\n\nval_article = train_article[t:]\nval_summary = train_summary[t:] \n\n# val_article,val_summary = resize(val_article,val_summary)\n\ndata_dict = {\n    \"Article\": val_article,\n    \"Summary\": val_summary\n}\n\n# Create a Dataset object from the dictionary\ndataset_val = Dataset.from_dict(data_dict)\n\ndict_val = DatasetDict({\n    'validation': dataset_val,\n    # Add more datasets for validation, test, etc., as needed\n})\n\nprint(type(dict_val))","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:03:06.346810Z","iopub.execute_input":"2024-04-24T23:03:06.347205Z","iopub.status.idle":"2024-04-24T23:03:06.516083Z","shell.execute_reply.started":"2024-04-24T23:03:06.347170Z","shell.execute_reply":"2024-04-24T23:03:06.515120Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"<class 'datasets.dataset_dict.DatasetDict'>\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/IndicBART\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"ai4bharat/IndicBART\")\n# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# tokenizer = AutoTokenizer.from_pretrained(\"Someman/bart-hindi\")\n# model = AutoModelForSeq2SeqLM.from_pretrained(\"Someman/bart-hindi\")\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:03:06.520147Z","iopub.execute_input":"2024-04-24T23:03:06.520481Z","iopub.status.idle":"2024-04-24T23:03:17.199240Z","shell.execute_reply.started":"2024-04-24T23:03:06.520453Z","shell.execute_reply":"2024-04-24T23:03:17.198150Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/498 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73e37cef1fc845a59741f809fa126a32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/832 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac18d9493ad34c02a5f59c129ef5fe1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.90M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d397f79144a44738adf756524bfa4450"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/221 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"021ad2e785d6497ab97a8cf0ef93e0a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/398 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90e1ff6783bb457ebfed3760f3777103"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/976M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f42dec4966c743839e43a0c7c6c40a71"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"MBartForConditionalGeneration(\n  (model): MBartModel(\n    (shared): Embedding(64014, 1024, padding_idx=0)\n    (encoder): MBartEncoder(\n      (embed_tokens): Embedding(64014, 1024, padding_idx=0)\n      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n      (layers): ModuleList(\n        (0-5): 6 x MBartEncoderLayer(\n          (self_attn): MBartAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): MBartDecoder(\n      (embed_tokens): Embedding(64014, 1024, padding_idx=0)\n      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n      (layers): ModuleList(\n        (0-5): 6 x MBartDecoderLayer(\n          (self_attn): MBartAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): MBartAttention(\n            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (lm_head): Linear(in_features=1024, out_features=64014, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\nimport torch\n\ndf_test = pd.read_csv('/kaggle/input/summariser-data2022/test.csv')\ntext= df_test['Article'].tolist()[12]\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\ninputs = tokenizer(text, return_tensors=\"pt\")\n\n# Move the input tensors to the same device as the model\ninputs = {k: v.to(device) for k, v in inputs.items()}\n\n# Generate outputs using the model\nwith torch.no_grad():\n    # Access the correct key for `input_ids` in the `inputs` dictionary\n    input_ids = inputs[\"input_ids\"]\n    outputs = model.generate(input_ids, max_new_tokens=70, do_sample=False)\n\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(\"Generated Text:\", generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:03:17.200592Z","iopub.execute_input":"2024-04-24T23:03:17.201079Z","iopub.status.idle":"2024-04-24T23:03:20.362399Z","shell.execute_reply.started":"2024-04-24T23:03:17.201052Z","shell.execute_reply":"2024-04-24T23:03:20.361276Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Generated Text: देहरादून लखनऊ. उत्तराखंड के चमोली जिला में बड़ी आपदा में उत्तर प्रदेश सरकार ने मदद के लिए अपने दरवाजे खोल दिए हैं। इसके साथ ही वहां पर फंसे लोगों को बाहर निकालने में सहयोग देने के साथ यूपी सरकार ने इमरजेंसी ऑपरेशन कंट्रोल रूम भी स्थापित किया। जिसके लिए हेल्पलाइन के साथ व्हाट्सएप नम्बर भी जारी किया गया है। मुख्यमंत्री योगी ने\n","output_type":"stream"}]},{"cell_type":"code","source":"summary = df_test['Summary'].tolist()[12]\nsummary","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:03:20.363521Z","iopub.execute_input":"2024-04-24T23:03:20.363801Z","iopub.status.idle":"2024-04-24T23:03:20.370878Z","shell.execute_reply.started":"2024-04-24T23:03:20.363777Z","shell.execute_reply":"2024-04-24T23:03:20.369854Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'सात फरवरी को आयी आपदा में यहां पर लखीमपुर खीरी के साथ ही सहारनपुर, अमरोहा, श्रावस्ती के साथ अन्य जिलों के सैकड़ों लोगों के फंसे होने की संभावना है। इस संबंध में राहत आयुक्त उत्तर प्रदेश लगातार उत्तराखंड सरकार के साथ समन्वय कर रहे हैं। किसी भी प्रकार की जानकारी मिलने पर पीड़ित से सम्पर्क किया जा रहा है।'"},"metadata":{}}]},{"cell_type":"code","source":"text","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:03:20.372402Z","iopub.execute_input":"2024-04-24T23:03:20.372789Z","iopub.status.idle":"2024-04-24T23:03:20.382018Z","shell.execute_reply.started":"2024-04-24T23:03:20.372753Z","shell.execute_reply":"2024-04-24T23:03:20.381033Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'लखनऊ. उत्तराखंड के चमोली जिला में बड़ी आपदा में उत्तर प्रदेश सरकार ने मदद के लिए अपने दरवाजे खोल दिए हैं। इसके साथ ही वहां पर फंसे लोगों को बाहर निकालने में सहयोग देने के साथ यूपी सरकार ने इमरजेंसी ऑपरेशन कंट्रोल रूम भी स्थापित किया। जिसके लिए हेल्पलाइन के साथ व्हाट्सएप नम्बर भी जारी किया गया है। मुख्यमंत्री योगी ने उत्तराखंड राज्य सरकार से समन्वय के लिए दो अधिकारियों को देहरादून भेजने के निर्देश दिए हैं। साथ ही गन्ना विकास मंत्री सुरेश राणा से उत्तराखंड सरकार से समन्वय स्थापित कर प्रदेश के प्रभावित और लापता लोगों की खोज की कार्रवाई के आदेश दिए हैं।पढ़ें- TMC सांसद महुआ मोइत्रा ने पूर्व सीजेआई की आलोचना कीसात फरवरी को आयी आपदा में यहां पर लखीमपुर खीरी के साथ ही सहारनपुर, अमरोहा, श्रावस्ती के साथ अन्य जिलों के सैकड़ों लोगों के फंसे होने की संभावना है। इस संबंध में राहत आयुक्त उत्तर प्रदेश लगातार उत्तराखंड सरकार के साथ समन्वय कर रहे हैं। किसी भी प्रकार की जानकारी मिलने पर पीड़ित से सम्पर्क किया जा रहा है। घायल या फिर चोटिल होने की स्थिति में उनके इलाज की व्यवस्था की जा रही है। इसके साथ भी लोगों को उनके घर वापस भेजने का भी इंतजाम किया जा रहा है।पढ़ें- चमोली तपोवन हादसे में लापता लोगों की लिस्ट, कई राज्यों के लोग शामिलआपदा में उत्तर प्रदेश वासियों की खोज-बचाव व उनके परिवारों से समन्वय के लिए राज्यस्तरीय इमरजेंसी अपरेशन सेंटर हर समय क्रियाशील है। इस हादसे के दौरान लापता व्यक्तियों के परिवार के लोग राहत हेल्पलाइन 1070 के साथ व्हाट्सएप नंबर 9454441036 पर उनका विवरण दर्ज करा सकते हैं। मुख्यमंत्री योगी आदित्यनाथ ने कहा कि देवभूमि उत्तराखंड में इस आपदा से निपटने के लिए उत्तर प्रदेश सरकार अपने पड़ोसी प्रदेश की हर संभव सहायता प्रदान करेगी।पढ़ें- Chamoli में जिंदगी बचाने की जंग जारी, देखिए रेस्क्यू ऑपरेशन की 10 तस्वीरेंमुख्यमंत्री योगी आदित्यनाथ ने इसके साथ ही प्रदेश में गंगा नदी के तट पर बसे 27 जिलों में जिलाधिकारी तथा एसपी को निर्देश दिया है कि नदी के घाट से सटे गांव तथा कस्बों में लोगों को लगातार सचेत करें। गंगा किनारे स्थित जिलों में जल स्तर की लगातार निगरानी की जा रही है। जल स्तर बढ़ा तो लोगों को वहां से अलग भेजने की तैयारी है। राहत और बचाव के निर्देश दे दिए गए हैं। मुख्यमंत्री ने लोगों से अपील की है कि वे किसी भी अफ वाह पर भरोसा न करें और न ही अफ वाह फैलाएं। खुद सतर्कता बरतते हुए नदी किनारे न जाएं। विषम परिस्थिति हो तो जिला प्रशासन के साथ सहयोग करें।पढ़ें- चमोली में पुल टूटने से अलग हो गए 13 गांव, बचाव कार्य जारी, पहुंचाई जा रही है राहत सामग्री'"},"metadata":{}}]},{"cell_type":"code","source":"max_input_length = 600\nmax_target_length = 40\n\nprefix=\"summarize: \"\nx=10\ndef preprocess_function(examples):\n    inputs = [prefix + doc for doc in examples[\"Article\"]]\n    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True,padding='max_length')\n\n    # Setup the tokenizer for targets\n    labels = tokenizer(text_target=examples[\"Summary\"], max_length=max_target_length, truncation=True,padding='max_length')\n#     labels = tokenizer(text_target=examples[\"Summary\"], padding='longest', truncation=True)\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:03:20.383176Z","iopub.execute_input":"2024-04-24T23:03:20.383559Z","iopub.status.idle":"2024-04-24T23:03:20.390773Z","shell.execute_reply.started":"2024-04-24T23:03:20.383524Z","shell.execute_reply":"2024-04-24T23:03:20.389754Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# inp = preprocess_function(dataset_train['train'])\n\n# x=10\n# tokenised_data_train = dataset_train.map(preprocess_function, batched=True)\n# tokenised_data_val = dataset_val.map(preprocess_function, batched=True)\n\ntokenised_data_train = dict_train.map(preprocess_function, batched=True)\ntokenised_data_val = dict_val.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:03:20.392266Z","iopub.execute_input":"2024-04-24T23:03:20.393152Z","iopub.status.idle":"2024-04-24T23:03:59.674735Z","shell.execute_reply.started":"2024-04-24T23:03:20.393115Z","shell.execute_reply":"2024-04-24T23:03:59.673680Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19102 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"459adf5528c14f6dab8c05d214f2a0c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2123 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a638cfa8e5154deaaa6604b2bef4533d"}},"metadata":{}}]},{"cell_type":"code","source":"type(tokenised_data_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:03:59.676113Z","iopub.execute_input":"2024-04-24T23:03:59.676498Z","iopub.status.idle":"2024-04-24T23:03:59.683703Z","shell.execute_reply.started":"2024-04-24T23:03:59.676465Z","shell.execute_reply":"2024-04-24T23:03:59.682715Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"datasets.dataset_dict.DatasetDict"},"metadata":{}}]},{"cell_type":"code","source":"import nltk\nimport numpy as np\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # Rouge expects a newline after each sentence\n    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n    \n    # Note that other metrics may not have a `use_aggregator` parameter\n    # and thus will return a list, computing a metric for each sentence.\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)\n    # Extract a few results\n    result = {key: value * 100 for key, value in result.items()}\n    \n    # Add mean generated length\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    \n    return {k: round(v, 4) for k, v in result.items()}","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:03:59.684949Z","iopub.execute_input":"2024-04-24T23:03:59.685250Z","iopub.status.idle":"2024-04-24T23:04:00.947890Z","shell.execute_reply.started":"2024-04-24T23:03:59.685225Z","shell.execute_reply":"2024-04-24T23:04:00.946953Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model='ai4bharat/IndicBART')","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:04:00.949117Z","iopub.execute_input":"2024-04-24T23:04:00.949458Z","iopub.status.idle":"2024-04-24T23:04:11.166811Z","shell.execute_reply.started":"2024-04-24T23:04:00.949430Z","shell.execute_reply":"2024-04-24T23:04:11.165877Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"2024-04-24 23:04:02.687690: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-24 23:04:02.687825: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-24 23:04:02.819376: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install rouge_score\n!pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:04:11.167967Z","iopub.execute_input":"2024-04-24T23:04:11.168253Z","iopub.status.idle":"2024-04-24T23:04:43.854216Z","shell.execute_reply.started":"2024-04-24T23:04:11.168228Z","shell.execute_reply":"2024-04-24T23:04:43.852826Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=59937413a54f320f37f3270b1c18406779975cf749b6d85b6310d19cd75c851a\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.18.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.2.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.22.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nCollecting responses<0.19 (from evaluate)\n  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading responses-0.18.0-py3-none-any.whl (38 kB)\nInstalling collected packages: responses, evaluate\nSuccessfully installed evaluate-0.4.1 responses-0.18.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\nimport evaluate\n\nrouge = evaluate.load(\"rouge\")\n\n\nbatch_size = 8\n# model_name = model_checkpoint.split(\"/\")[-1]\nargs = Seq2SeqTrainingArguments(\n    output_dir=\"Indic_Bart\",\n#     eval_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    weight_decay=0.01,\n    save_total_limit=5,\n    num_train_epochs=2,\n    predict_with_generate=True,\n    fp16=True,\n)\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:04:43.856018Z","iopub.execute_input":"2024-04-24T23:04:43.856366Z","iopub.status.idle":"2024-04-24T23:04:44.563746Z","shell.execute_reply.started":"2024-04-24T23:04:43.856335Z","shell.execute_reply":"2024-04-24T23:04:44.562655Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e49617e8bcfa40319de873d3ed7e769e"}},"metadata":{}}]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:04:44.565050Z","iopub.execute_input":"2024-04-24T23:04:44.565350Z","iopub.status.idle":"2024-04-24T23:04:44.586823Z","shell.execute_reply.started":"2024-04-24T23:04:44.565324Z","shell.execute_reply":"2024-04-24T23:04:44.585569Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"tokenised_data_train","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:04:44.588384Z","iopub.execute_input":"2024-04-24T23:04:44.588885Z","iopub.status.idle":"2024-04-24T23:04:44.599247Z","shell.execute_reply.started":"2024-04-24T23:04:44.588844Z","shell.execute_reply":"2024-04-24T23:04:44.598152Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['Article', 'Summary', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 19102\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"trainer = Seq2SeqTrainer(\n    model,\n    args,\n    train_dataset=tokenised_data_train[\"train\"],\n    eval_dataset=tokenised_data_val[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics\n#      data_collator=data_collator,\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-24T23:04:44.600647Z","iopub.execute_input":"2024-04-24T23:04:44.601001Z","iopub.status.idle":"2024-04-25T00:07:24.255927Z","shell.execute_reply.started":"2024-04-24T23:04:44.600972Z","shell.execute_reply":"2024-04-25T00:07:24.254904Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240424_231601-6bpfo0th</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/iiitd-ed/huggingface/runs/6bpfo0th' target=\"_blank\">comfy-planet-41</a></strong> to <a href='https://wandb.ai/iiitd-ed/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/iiitd-ed/huggingface' target=\"_blank\">https://wandb.ai/iiitd-ed/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/iiitd-ed/huggingface/runs/6bpfo0th' target=\"_blank\">https://wandb.ai/iiitd-ed/huggingface/runs/6bpfo0th</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2388' max='2388' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2388/2388 51:02, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>2.655100</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>2.151800</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.935300</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.840500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2388, training_loss=2.091841731239204, metrics={'train_runtime': 3758.2463, 'train_samples_per_second': 10.165, 'train_steps_per_second': 0.635, 'total_flos': 2.42563325755392e+16, 'train_loss': 2.091841731239204, 'epoch': 2.0})"},"metadata":{}}]},{"cell_type":"code","source":"print()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T00:07:24.258193Z","iopub.execute_input":"2024-04-25T00:07:24.259415Z","iopub.status.idle":"2024-04-25T00:07:24.264748Z","shell.execute_reply.started":"2024-04-25T00:07:24.259385Z","shell.execute_reply":"2024-04-25T00:07:24.263755Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"text = df_train['Article'].tolist()[11]\ntext='summarise: '+text\ntext","metadata":{"execution":{"iopub.status.busy":"2024-04-25T00:07:24.266141Z","iopub.execute_input":"2024-04-25T00:07:24.266789Z","iopub.status.idle":"2024-04-25T00:07:24.311848Z","shell.execute_reply.started":"2024-04-25T00:07:24.266753Z","shell.execute_reply":"2024-04-25T00:07:24.310419Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"'summarise: सरकारी नौकरी की तलाश में जुटे युवाओं के लिए हम फिर से 5 लेटेस्ट नौकरियों की जानकारी के साथ हाजिर हैं। राजस्थान हाईकोर्ट में स्टेनोग्राफर के 277 पदों पर वैकेंसी है। आवेदन की शुरुआत एक अगस्त 2023 से होगी।\\nरेल इंडिया टेक्निकल एंड इकोनॉमिक सर्विस (RITES) ने भारत के रेल मंत्रालय के तहत ड्राफ्ट्समैन सिविल इंजीनियर, पर्यावरण सामाजिक निगरानी विशेषज्ञ, जूनियर डिजाइन इंजीनियर के 111 पदों के लिए ऑनलाइन आवेदन शुरू कर दिया है। 7 अगस्त तक ऑनलाइन आवेदन कर सकते हैं।\\nभुवनेश्वर AIIMS में सीनियर नर्सिंग ऑफिसर, स्टोर कीपर, कैशियर, कनिष्ठ प्रशासनिक सहायक, मेडिकल रिकॉर्ड तकनीशियन (रिकॉर्ड क्लर्क), वायरमैन, फार्मासिस्ट के 775 पदों के लिए वैकेंसी थी। अगर आप अभी तक अप्लाई नहीं कर सके हैं तो आज यानी 30 जुलाई लास्ट डेट है।\\nBHU यानी बनारस हिंदू विश्वविद्यालय ने असिस्टेंट प्रोफेसर, एसोसिएट प्रोफेसर और प्रोफेसर के 307 पदों पर भर्ती निकाली है। अप्लाई करने की लास्ट डेट कल यानी 31 जुलाई 2023 है।\\nनेत्र परीक्षण अधिकारी के 157 पदों पर भर्ती निकली है। आवेदन की आखिरी डेट 7 अगस्त 2023 है। सिलेक्शन होने पर 25,500 रुपए से 81,100 रुपए हर महीना सैलरी मिलेगी।\\nआइए, 5 नौकरियों की डिटेल्ड जानकारी आगे 5 ग्राफिक्स में जानते हैं। छठे ग्राफिक में करेंट अफेयर्स के 10 सवाल और उनके जवाब होंगे।\\nआपने यहां 5 नौकरियों के बारे में जाना। आपके मन में कुछ सवाल होंगे। इसलिए आप दिए गए वेबसाइट के जरिए ऑफिशियल नोटिफिकेशन को जरूर देखें। बाकी जिन नौकरियों के बारे में बताया गया है, अगर लगता है कि इससे आपके भाई-दोस्त या फिर रिश्तेदार की जरूरत पूरी होती है तो उन्हें यह खबर जरूर भेजें।\\nआखिर में हम 10 लेटेस्ट करेंट अफेयर्स के सवाल-जवाब दे रहे हैं। इन्हें रोज देखिए। हो सके तो सेव करते जाएं। ताकि आगामी परीक्षाओं में यह आपके लिए फायदेमंद साबित हों।'"},"metadata":{}}]},{"cell_type":"code","source":"import pickle\nwith open('Indic_Bart.pkl', 'wb') as f:\n    pickle.dump(model, f)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T00:07:24.313497Z","iopub.execute_input":"2024-04-25T00:07:24.313915Z","iopub.status.idle":"2024-04-25T00:07:26.481404Z","shell.execute_reply.started":"2024-04-25T00:07:24.313873Z","shell.execute_reply":"2024-04-25T00:07:26.480344Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"tokenizer.save_pretrained('tokeniser')","metadata":{"execution":{"iopub.status.busy":"2024-04-25T00:07:26.482990Z","iopub.execute_input":"2024-04-25T00:07:26.483336Z","iopub.status.idle":"2024-04-25T00:07:26.523642Z","shell.execute_reply.started":"2024-04-25T00:07:26.483281Z","shell.execute_reply":"2024-04-25T00:07:26.522496Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"('tokeniser/tokenizer_config.json',\n 'tokeniser/special_tokens_map.json',\n 'tokeniser/spiece.model',\n 'tokeniser/added_tokens.json',\n 'tokeniser/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\n\ndf_test = pd.read_csv('/kaggle/input/summariser-data2022/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-25T00:07:26.530866Z","iopub.execute_input":"2024-04-25T00:07:26.531187Z","iopub.status.idle":"2024-04-25T00:07:27.003866Z","shell.execute_reply.started":"2024-04-25T00:07:26.531158Z","shell.execute_reply":"2024-04-25T00:07:27.002717Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\n# prin\nwith open('/kaggle/working/Indic_Bart.pkl', 'rb') as f:\n    model = pickle.load(f)\ntext= df_test['Article'].tolist()[12]\nsummarizer = pipeline(\"summarization\", model=model,tokenizer=AutoTokenizer.from_pretrained(\"/kaggle/working/tokeniser\"))\nsummarizer(text)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T00:07:27.005326Z","iopub.execute_input":"2024-04-25T00:07:27.005736Z","iopub.status.idle":"2024-04-25T00:07:30.230010Z","shell.execute_reply.started":"2024-04-25T00:07:27.005696Z","shell.execute_reply":"2024-04-25T00:07:30.228840Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"[{'summary_text': 'उत्तर प्रदेश सरकार ने मदद के लिए अपने दरवाजे खोल दिए हैं। इसके साथ ही वहां'}]"},"metadata":{}}]},{"cell_type":"code","source":"text","metadata":{"execution":{"iopub.status.busy":"2024-04-25T00:07:30.231630Z","iopub.execute_input":"2024-04-25T00:07:30.232174Z","iopub.status.idle":"2024-04-25T00:07:30.240893Z","shell.execute_reply.started":"2024-04-25T00:07:30.232132Z","shell.execute_reply":"2024-04-25T00:07:30.239641Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"'लखनऊ. उत्तराखंड के चमोली जिला में बड़ी आपदा में उत्तर प्रदेश सरकार ने मदद के लिए अपने दरवाजे खोल दिए हैं। इसके साथ ही वहां पर फंसे लोगों को बाहर निकालने में सहयोग देने के साथ यूपी सरकार ने इमरजेंसी ऑपरेशन कंट्रोल रूम भी स्थापित किया। जिसके लिए हेल्पलाइन के साथ व्हाट्सएप नम्बर भी जारी किया गया है। मुख्यमंत्री योगी ने उत्तराखंड राज्य सरकार से समन्वय के लिए दो अधिकारियों को देहरादून भेजने के निर्देश दिए हैं। साथ ही गन्ना विकास मंत्री सुरेश राणा से उत्तराखंड सरकार से समन्वय स्थापित कर प्रदेश के प्रभावित और लापता लोगों की खोज की कार्रवाई के आदेश दिए हैं।पढ़ें- TMC सांसद महुआ मोइत्रा ने पूर्व सीजेआई की आलोचना कीसात फरवरी को आयी आपदा में यहां पर लखीमपुर खीरी के साथ ही सहारनपुर, अमरोहा, श्रावस्ती के साथ अन्य जिलों के सैकड़ों लोगों के फंसे होने की संभावना है। इस संबंध में राहत आयुक्त उत्तर प्रदेश लगातार उत्तराखंड सरकार के साथ समन्वय कर रहे हैं। किसी भी प्रकार की जानकारी मिलने पर पीड़ित से सम्पर्क किया जा रहा है। घायल या फिर चोटिल होने की स्थिति में उनके इलाज की व्यवस्था की जा रही है। इसके साथ भी लोगों को उनके घर वापस भेजने का भी इंतजाम किया जा रहा है।पढ़ें- चमोली तपोवन हादसे में लापता लोगों की लिस्ट, कई राज्यों के लोग शामिलआपदा में उत्तर प्रदेश वासियों की खोज-बचाव व उनके परिवारों से समन्वय के लिए राज्यस्तरीय इमरजेंसी अपरेशन सेंटर हर समय क्रियाशील है। इस हादसे के दौरान लापता व्यक्तियों के परिवार के लोग राहत हेल्पलाइन 1070 के साथ व्हाट्सएप नंबर 9454441036 पर उनका विवरण दर्ज करा सकते हैं। मुख्यमंत्री योगी आदित्यनाथ ने कहा कि देवभूमि उत्तराखंड में इस आपदा से निपटने के लिए उत्तर प्रदेश सरकार अपने पड़ोसी प्रदेश की हर संभव सहायता प्रदान करेगी।पढ़ें- Chamoli में जिंदगी बचाने की जंग जारी, देखिए रेस्क्यू ऑपरेशन की 10 तस्वीरेंमुख्यमंत्री योगी आदित्यनाथ ने इसके साथ ही प्रदेश में गंगा नदी के तट पर बसे 27 जिलों में जिलाधिकारी तथा एसपी को निर्देश दिया है कि नदी के घाट से सटे गांव तथा कस्बों में लोगों को लगातार सचेत करें। गंगा किनारे स्थित जिलों में जल स्तर की लगातार निगरानी की जा रही है। जल स्तर बढ़ा तो लोगों को वहां से अलग भेजने की तैयारी है। राहत और बचाव के निर्देश दे दिए गए हैं। मुख्यमंत्री ने लोगों से अपील की है कि वे किसी भी अफ वाह पर भरोसा न करें और न ही अफ वाह फैलाएं। खुद सतर्कता बरतते हुए नदी किनारे न जाएं। विषम परिस्थिति हो तो जिला प्रशासन के साथ सहयोग करें।पढ़ें- चमोली में पुल टूटने से अलग हो गए 13 गांव, बचाव कार्य जारी, पहुंचाई जा रही है राहत सामग्री'"},"metadata":{}}]},{"cell_type":"code","source":"t=df_test['Summary'].tolist()[12]\nt","metadata":{"execution":{"iopub.status.busy":"2024-04-25T00:07:30.242268Z","iopub.execute_input":"2024-04-25T00:07:30.242687Z","iopub.status.idle":"2024-04-25T00:07:30.252364Z","shell.execute_reply.started":"2024-04-25T00:07:30.242646Z","shell.execute_reply":"2024-04-25T00:07:30.251182Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"'सात फरवरी को आयी आपदा में यहां पर लखीमपुर खीरी के साथ ही सहारनपुर, अमरोहा, श्रावस्ती के साथ अन्य जिलों के सैकड़ों लोगों के फंसे होने की संभावना है। इस संबंध में राहत आयुक्त उत्तर प्रदेश लगातार उत्तराखंड सरकार के साथ समन्वय कर रहे हैं। किसी भी प्रकार की जानकारी मिलने पर पीड़ित से सम्पर्क किया जा रहा है।'"},"metadata":{}}]},{"cell_type":"code","source":"with open('/kaggle/working/Indic_Bart.pkl', 'rb') as f:\n    model = pickle.load(f)\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/tokeniser\")","metadata":{"execution":{"iopub.status.busy":"2024-04-25T00:07:30.253726Z","iopub.execute_input":"2024-04-25T00:07:30.254749Z","iopub.status.idle":"2024-04-25T00:07:31.921575Z","shell.execute_reply.started":"2024-04-25T00:07:30.254711Z","shell.execute_reply":"2024-04-25T00:07:31.920340Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\ninputs = tokenizer(text, return_tensors=\"pt\")\n\n# Move the input tensors to the same device as the model\ninputs = {k: v.to(device) for k, v in inputs.items()}\n\n# Generate outputs using the model\nwith torch.no_grad():\n    # Access the correct key for `input_ids` in the `inputs` dictionary\n    input_ids = inputs[\"input_ids\"]\n    outputs = model.generate(input_ids, max_new_tokens=100, do_sample=False)\n\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(\"Generated Text:\", generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T00:07:31.923127Z","iopub.execute_input":"2024-04-25T00:07:31.923592Z","iopub.status.idle":"2024-04-25T00:07:32.937207Z","shell.execute_reply.started":"2024-04-25T00:07:31.923550Z","shell.execute_reply":"2024-04-25T00:07:32.936093Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Generated Text: उत्तराखंड के चमोली जिला में बड़ी आपदा में उत्तर प्रदेश सरकार ने मदद के लिए अपने दरवाजे खोल दिए हैं। इसके साथ ही वहां पर फंसे लोगों को बाहर निकालने में सहयोग देने के साथ यूपी सरकार ने इमरजेंसी ऑपरेशन कंट्रोल रूम भी स्थापित किया। जिसके लिए हेल्पलाइन के साथ व्हाट्सएप नम्बर भी जारी किया गया है। मुख्यमंत्री योगी ने उत्तराखंड राज्य सरकार से समन्वय के लिए दो अधिकारियों को देहरादून भेजने के निर्देश दिए हैं। साथ ही लोगों को उनके घर वापस भेजने का भी इंतजाम किया जा रहा है\n","output_type":"stream"}]},{"cell_type":"code","source":"text","metadata":{"execution":{"iopub.status.busy":"2024-04-25T00:07:32.938801Z","iopub.execute_input":"2024-04-25T00:07:32.939218Z","iopub.status.idle":"2024-04-25T00:07:32.947175Z","shell.execute_reply.started":"2024-04-25T00:07:32.939180Z","shell.execute_reply":"2024-04-25T00:07:32.946129Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"'लखनऊ. उत्तराखंड के चमोली जिला में बड़ी आपदा में उत्तर प्रदेश सरकार ने मदद के लिए अपने दरवाजे खोल दिए हैं। इसके साथ ही वहां पर फंसे लोगों को बाहर निकालने में सहयोग देने के साथ यूपी सरकार ने इमरजेंसी ऑपरेशन कंट्रोल रूम भी स्थापित किया। जिसके लिए हेल्पलाइन के साथ व्हाट्सएप नम्बर भी जारी किया गया है। मुख्यमंत्री योगी ने उत्तराखंड राज्य सरकार से समन्वय के लिए दो अधिकारियों को देहरादून भेजने के निर्देश दिए हैं। साथ ही गन्ना विकास मंत्री सुरेश राणा से उत्तराखंड सरकार से समन्वय स्थापित कर प्रदेश के प्रभावित और लापता लोगों की खोज की कार्रवाई के आदेश दिए हैं।पढ़ें- TMC सांसद महुआ मोइत्रा ने पूर्व सीजेआई की आलोचना कीसात फरवरी को आयी आपदा में यहां पर लखीमपुर खीरी के साथ ही सहारनपुर, अमरोहा, श्रावस्ती के साथ अन्य जिलों के सैकड़ों लोगों के फंसे होने की संभावना है। इस संबंध में राहत आयुक्त उत्तर प्रदेश लगातार उत्तराखंड सरकार के साथ समन्वय कर रहे हैं। किसी भी प्रकार की जानकारी मिलने पर पीड़ित से सम्पर्क किया जा रहा है। घायल या फिर चोटिल होने की स्थिति में उनके इलाज की व्यवस्था की जा रही है। इसके साथ भी लोगों को उनके घर वापस भेजने का भी इंतजाम किया जा रहा है।पढ़ें- चमोली तपोवन हादसे में लापता लोगों की लिस्ट, कई राज्यों के लोग शामिलआपदा में उत्तर प्रदेश वासियों की खोज-बचाव व उनके परिवारों से समन्वय के लिए राज्यस्तरीय इमरजेंसी अपरेशन सेंटर हर समय क्रियाशील है। इस हादसे के दौरान लापता व्यक्तियों के परिवार के लोग राहत हेल्पलाइन 1070 के साथ व्हाट्सएप नंबर 9454441036 पर उनका विवरण दर्ज करा सकते हैं। मुख्यमंत्री योगी आदित्यनाथ ने कहा कि देवभूमि उत्तराखंड में इस आपदा से निपटने के लिए उत्तर प्रदेश सरकार अपने पड़ोसी प्रदेश की हर संभव सहायता प्रदान करेगी।पढ़ें- Chamoli में जिंदगी बचाने की जंग जारी, देखिए रेस्क्यू ऑपरेशन की 10 तस्वीरेंमुख्यमंत्री योगी आदित्यनाथ ने इसके साथ ही प्रदेश में गंगा नदी के तट पर बसे 27 जिलों में जिलाधिकारी तथा एसपी को निर्देश दिया है कि नदी के घाट से सटे गांव तथा कस्बों में लोगों को लगातार सचेत करें। गंगा किनारे स्थित जिलों में जल स्तर की लगातार निगरानी की जा रही है। जल स्तर बढ़ा तो लोगों को वहां से अलग भेजने की तैयारी है। राहत और बचाव के निर्देश दे दिए गए हैं। मुख्यमंत्री ने लोगों से अपील की है कि वे किसी भी अफ वाह पर भरोसा न करें और न ही अफ वाह फैलाएं। खुद सतर्कता बरतते हुए नदी किनारे न जाएं। विषम परिस्थिति हो तो जिला प्रशासन के साथ सहयोग करें।पढ़ें- चमोली में पुल टूटने से अलग हो गए 13 गांव, बचाव कार्य जारी, पहुंचाई जा रही है राहत सामग्री'"},"metadata":{}}]},{"cell_type":"code","source":"t=df_test['Summary'].tolist()[12]\nt","metadata":{"execution":{"iopub.status.busy":"2024-04-25T00:07:32.948960Z","iopub.execute_input":"2024-04-25T00:07:32.949727Z","iopub.status.idle":"2024-04-25T00:07:32.958346Z","shell.execute_reply.started":"2024-04-25T00:07:32.949697Z","shell.execute_reply":"2024-04-25T00:07:32.957160Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"'सात फरवरी को आयी आपदा में यहां पर लखीमपुर खीरी के साथ ही सहारनपुर, अमरोहा, श्रावस्ती के साथ अन्य जिलों के सैकड़ों लोगों के फंसे होने की संभावना है। इस संबंध में राहत आयुक्त उत्तर प्रदेश लगातार उत्तराखंड सरकार के साथ समन्वय कर रहे हैं। किसी भी प्रकार की जानकारी मिलने पर पीड़ित से सम्पर्क किया जा रहा है।'"},"metadata":{}}]},{"cell_type":"code","source":"text","metadata":{"execution":{"iopub.status.busy":"2024-04-25T00:07:32.959702Z","iopub.execute_input":"2024-04-25T00:07:32.960113Z","iopub.status.idle":"2024-04-25T00:07:32.969799Z","shell.execute_reply.started":"2024-04-25T00:07:32.960086Z","shell.execute_reply":"2024-04-25T00:07:32.968099Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"'लखनऊ. उत्तराखंड के चमोली जिला में बड़ी आपदा में उत्तर प्रदेश सरकार ने मदद के लिए अपने दरवाजे खोल दिए हैं। इसके साथ ही वहां पर फंसे लोगों को बाहर निकालने में सहयोग देने के साथ यूपी सरकार ने इमरजेंसी ऑपरेशन कंट्रोल रूम भी स्थापित किया। जिसके लिए हेल्पलाइन के साथ व्हाट्सएप नम्बर भी जारी किया गया है। मुख्यमंत्री योगी ने उत्तराखंड राज्य सरकार से समन्वय के लिए दो अधिकारियों को देहरादून भेजने के निर्देश दिए हैं। साथ ही गन्ना विकास मंत्री सुरेश राणा से उत्तराखंड सरकार से समन्वय स्थापित कर प्रदेश के प्रभावित और लापता लोगों की खोज की कार्रवाई के आदेश दिए हैं।पढ़ें- TMC सांसद महुआ मोइत्रा ने पूर्व सीजेआई की आलोचना कीसात फरवरी को आयी आपदा में यहां पर लखीमपुर खीरी के साथ ही सहारनपुर, अमरोहा, श्रावस्ती के साथ अन्य जिलों के सैकड़ों लोगों के फंसे होने की संभावना है। इस संबंध में राहत आयुक्त उत्तर प्रदेश लगातार उत्तराखंड सरकार के साथ समन्वय कर रहे हैं। किसी भी प्रकार की जानकारी मिलने पर पीड़ित से सम्पर्क किया जा रहा है। घायल या फिर चोटिल होने की स्थिति में उनके इलाज की व्यवस्था की जा रही है। इसके साथ भी लोगों को उनके घर वापस भेजने का भी इंतजाम किया जा रहा है।पढ़ें- चमोली तपोवन हादसे में लापता लोगों की लिस्ट, कई राज्यों के लोग शामिलआपदा में उत्तर प्रदेश वासियों की खोज-बचाव व उनके परिवारों से समन्वय के लिए राज्यस्तरीय इमरजेंसी अपरेशन सेंटर हर समय क्रियाशील है। इस हादसे के दौरान लापता व्यक्तियों के परिवार के लोग राहत हेल्पलाइन 1070 के साथ व्हाट्सएप नंबर 9454441036 पर उनका विवरण दर्ज करा सकते हैं। मुख्यमंत्री योगी आदित्यनाथ ने कहा कि देवभूमि उत्तराखंड में इस आपदा से निपटने के लिए उत्तर प्रदेश सरकार अपने पड़ोसी प्रदेश की हर संभव सहायता प्रदान करेगी।पढ़ें- Chamoli में जिंदगी बचाने की जंग जारी, देखिए रेस्क्यू ऑपरेशन की 10 तस्वीरेंमुख्यमंत्री योगी आदित्यनाथ ने इसके साथ ही प्रदेश में गंगा नदी के तट पर बसे 27 जिलों में जिलाधिकारी तथा एसपी को निर्देश दिया है कि नदी के घाट से सटे गांव तथा कस्बों में लोगों को लगातार सचेत करें। गंगा किनारे स्थित जिलों में जल स्तर की लगातार निगरानी की जा रही है। जल स्तर बढ़ा तो लोगों को वहां से अलग भेजने की तैयारी है। राहत और बचाव के निर्देश दे दिए गए हैं। मुख्यमंत्री ने लोगों से अपील की है कि वे किसी भी अफ वाह पर भरोसा न करें और न ही अफ वाह फैलाएं। खुद सतर्कता बरतते हुए नदी किनारे न जाएं। विषम परिस्थिति हो तो जिला प्रशासन के साथ सहयोग करें।पढ़ें- चमोली में पुल टूटने से अलग हो गए 13 गांव, बचाव कार्य जारी, पहुंचाई जा रही है राहत सामग्री'"},"metadata":{}}]},{"cell_type":"code","source":"!pip install indic-nlp-library\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T00:07:32.971248Z","iopub.execute_input":"2024-04-25T00:07:32.971643Z","iopub.status.idle":"2024-04-25T00:07:51.899072Z","shell.execute_reply.started":"2024-04-25T00:07:32.971609Z","shell.execute_reply":"2024-04-25T00:07:51.897854Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting indic-nlp-library\n  Downloading indic_nlp_library-0.92-py3-none-any.whl.metadata (5.7 kB)\nCollecting sphinx-argparse (from indic-nlp-library)\n  Downloading sphinx_argparse-0.4.0-py3-none-any.whl.metadata (1.9 kB)\nRequirement already satisfied: sphinx-rtd-theme in /opt/conda/lib/python3.10/site-packages (from indic-nlp-library) (0.2.4)\nCollecting morfessor (from indic-nlp-library)\n  Downloading Morfessor-2.0.6-py3-none-any.whl.metadata (628 bytes)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from indic-nlp-library) (2.1.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from indic-nlp-library) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->indic-nlp-library) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->indic-nlp-library) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->indic-nlp-library) (2023.4)\nCollecting sphinx>=1.2.0 (from sphinx-argparse->indic-nlp-library)\n  Downloading sphinx-7.3.7-py3-none-any.whl.metadata (6.0 kB)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.16.0)\nCollecting sphinxcontrib-applehelp (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library)\n  Downloading sphinxcontrib_applehelp-1.0.8-py3-none-any.whl.metadata (2.3 kB)\nCollecting sphinxcontrib-devhelp (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library)\n  Downloading sphinxcontrib_devhelp-1.0.6-py3-none-any.whl.metadata (2.3 kB)\nCollecting sphinxcontrib-jsmath (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library)\n  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl.metadata (1.4 kB)\nCollecting sphinxcontrib-htmlhelp>=2.0.0 (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library)\n  Downloading sphinxcontrib_htmlhelp-2.0.5-py3-none-any.whl.metadata (2.3 kB)\nCollecting sphinxcontrib-serializinghtml>=1.1.9 (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library)\n  Downloading sphinxcontrib_serializinghtml-1.1.10-py3-none-any.whl.metadata (2.4 kB)\nCollecting sphinxcontrib-qthelp (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library)\n  Downloading sphinxcontrib_qthelp-1.0.7-py3-none-any.whl.metadata (2.2 kB)\nRequirement already satisfied: Jinja2>=3.0 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.1.2)\nRequirement already satisfied: Pygments>=2.14 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.17.2)\nRequirement already satisfied: docutils<0.22,>=0.18.1 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.21.1)\nRequirement already satisfied: snowballstemmer>=2.0 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.2.0)\nRequirement already satisfied: babel>=2.9 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.14.0)\nCollecting alabaster~=0.7.14 (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library)\n  Downloading alabaster-0.7.16-py3-none-any.whl.metadata (2.9 kB)\nCollecting imagesize>=1.3 (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library)\n  Downloading imagesize-1.4.1-py2.py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: requests>=2.25.0 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.31.0)\nRequirement already satisfied: packaging>=21.0 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (21.3)\nRequirement already satisfied: tomli>=2 in /opt/conda/lib/python3.10/site-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2>=3.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.1.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.25.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2024.2.2)\nDownloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\nDownloading sphinx_argparse-0.4.0-py3-none-any.whl (12 kB)\nDownloading sphinx-7.3.7-py3-none-any.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading alabaster-0.7.16-py3-none-any.whl (13 kB)\nDownloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\nDownloading sphinxcontrib_htmlhelp-2.0.5-py3-none-any.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sphinxcontrib_serializinghtml-1.1.10-py3-none-any.whl (92 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.7/92.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sphinxcontrib_applehelp-1.0.8-py3-none-any.whl (120 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sphinxcontrib_devhelp-1.0.6-py3-none-any.whl (83 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\nDownloading sphinxcontrib_qthelp-1.0.7-py3-none-any.whl (89 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.4/89.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: morfessor, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, imagesize, alabaster, sphinx, sphinx-argparse, indic-nlp-library\nSuccessfully installed alabaster-0.7.16 imagesize-1.4.1 indic-nlp-library-0.92 morfessor-2.0.6 sphinx-7.3.7 sphinx-argparse-0.4.0 sphinxcontrib-applehelp-1.0.8 sphinxcontrib-devhelp-1.0.6 sphinxcontrib-htmlhelp-2.0.5 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.7 sphinxcontrib-serializinghtml-1.1.10\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\nimport torch\n\n# with open('/kaggle/working/Indic_Bart.pkl', 'rb') as f:\n#     model = pickle.load(f)\n# tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/tokeniser\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# def summarise(s):\n\ninputs = tokenizer(text, return_tensors=\"pt\")\n\n# Move the input tensors to the same device as the model\ninputs = {k: v.to(device) for k, v in inputs.items()}\n\n# Generate outputs using the model\nwith torch.no_grad():\n    # Access the correct key for `input_ids` in the `inputs` dictionary\n    input_ids = inputs[\"input_ids\"]\n    outputs = model.generate(input_ids, max_new_tokens=80, do_sample=False)\n\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n# return generated_text\nprint(\"Generated Text:\", generated_text)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T00:07:51.901259Z","iopub.execute_input":"2024-04-25T00:07:51.902236Z","iopub.status.idle":"2024-04-25T00:07:52.738656Z","shell.execute_reply.started":"2024-04-25T00:07:51.902189Z","shell.execute_reply":"2024-04-25T00:07:52.737206Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Generated Text: उत्तराखंड के चमोली जिला में बड़ी आपदा में उत्तर प्रदेश सरकार ने मदद के लिए अपने दरवाजे खोल दिए हैं। इसके साथ ही वहां पर फंसे लोगों को बाहर निकालने में सहयोग देने के साथ ही सहारनपुर, अमरोहा, श्रावस्ती के साथ अन्य जिलों के सैकड़ों लोगों के फंसे होने की संभावना है। इस संबंध में राहत आयुक्त उत्तर प्रदेश लगातार उत्तराखंड सरकार के साथ समन्वय कर रहे हैं।\n","output_type":"stream"}]},{"cell_type":"code","source":"summaries = []\nori_summary = []\ns=df_test['Summary'].tolist()\na = df_test['Article'].tolist()\nprint(len(s))\nori_summary.append(s[0])\nprint(a[0])\n# summaries.append(summarise(a[0]))\n# for i in range(len(s)):\n#     ori_summary.append(s[i])\n#     summaries.append(summarise(a[i]))","metadata":{"execution":{"iopub.status.busy":"2024-04-25T00:07:52.739981Z","iopub.execute_input":"2024-04-25T00:07:52.740322Z","iopub.status.idle":"2024-04-25T00:07:52.755376Z","shell.execute_reply.started":"2024-04-25T00:07:52.740267Z","shell.execute_reply":"2024-04-25T00:07:52.754111Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"2842\nनई दिल्ली. भारतीय जनता पार्टी ने अगले साल होने वाले चुनावों के चुनाव प्रभारियों की नियुक्तियां की हैं। भाजपा के लिहाज से सबसे महत्वपूर्ण राज्य उत्तर प्रदेश का जिम्मा धर्मेंद्र प्रधान को दिया है। धर्मेंद्र प्रधान यूपी के चुनाव प्रभारी होंगे। गजेंद्र सिंह शेखावत को पंजाब का प्रभारी बनाया है।उत्तर प्रदेश में भाजपा ने चुनाव के लिए अनुराग ठाकुर, अर्जुन मेघवाल, सरोज पांडेय, शोभा करंदलाजे, कैप्टन अभिमन्यु, अन्नपूर्णा देवी, विवेक ठाकुर को सह प्रभारी की जिम्मेदारी दी है। भाजपा ने उत्तर प्रदेश चुनाव के लिए राज्य में कई संगठन प्रभारी नियुक्त किए हैं। संजय भाटिया को वेस्टर्न यूपी, वाय. सत्या ठाकुर को अवध, संजीव चौरसिया को बृज, सुधीर गुप्ता को कानपुर, सुनील ओझा को काशी का संगठन प्रभारी नियुक्त किया गया है।केंद्रीय जल शक्ति मंत्री गजेंद्र सिंह शेखावत को पंजाब का प्रभारी बनाया गया है। पंजाब में हरदीप सिंह पुरी, मीनाक्षी लेखी और विनोद चावड़ा सह-प्रभारी नियुक्त किए गए हैं। हिंदी भाषी राज्य उत्तराखंड के लिए प्रल्हाद जोशी को प्रभारी बनाया गया है। प्रल्हाद जोशी केंद्र सरकार में कोयला और खान मंत्री हैं। उत्तराखंड में बंगाल से सांसद लॉकेट चटर्जी और सरदार आरपी सिंह सह प्रभारियों को रूप में उनका सहयोग करेंगे। केंद्रीय मंत्री भूपेंद्र यादव को मणिपुर का चुनाव प्रभारी नियुक्त किया गया है जबकि केंद्रीय मंत्री प्रतिमा भौमिक और असम सरकार के मंत्री अशोक सिंघल को यहां का सह प्रभारी बनाया गया है। वर्ष 2022 की शुरुआत में उत्तर प्रदेश, पंजाब, उत्तराखंड और मणिपुर में विधानसभा चुनाव होने हैं।\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install rouge","metadata":{"execution":{"iopub.status.busy":"2024-04-25T00:07:52.756938Z","iopub.execute_input":"2024-04-25T00:07:52.757356Z","iopub.status.idle":"2024-04-25T00:08:07.119619Z","shell.execute_reply.started":"2024-04-25T00:07:52.757318Z","shell.execute_reply":"2024-04-25T00:08:07.118159Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge\n  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge) (1.16.0)\nDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\nInstalling collected packages: rouge\nSuccessfully installed rouge-1.0.1\n","output_type":"stream"}]},{"cell_type":"code","source":"df_train=df_test[:100]","metadata":{"execution":{"iopub.status.busy":"2024-04-25T00:08:07.121712Z","iopub.execute_input":"2024-04-25T00:08:07.122670Z","iopub.status.idle":"2024-04-25T00:08:07.179767Z","shell.execute_reply.started":"2024-04-25T00:08:07.122620Z","shell.execute_reply":"2024-04-25T00:08:07.178190Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"\ndef predict_summary(text):\n    inputs = tokenizer(text, return_tensors='pt', max_length=600, truncation=True)\n    inputs.to(\"cuda\")\n    summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=40, early_stopping=True)\n    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    return summary\n\ndf_train['predicted_summary'] = df_train['Article'].apply(predict_summary)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T00:08:07.181250Z","iopub.execute_input":"2024-04-25T00:08:07.181654Z","iopub.status.idle":"2024-04-25T00:08:54.675952Z","shell.execute_reply.started":"2024-04-25T00:08:07.181616Z","shell.execute_reply":"2024-04-25T00:08:54.674920Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/1709492961.py:8: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_train['predicted_summary'] = df_train['Article'].apply(predict_summary)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pickle\nwith open('Indic_Bart.pkl', 'wb') as f:\n    pickle.dump(model, f)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('tokenizer.pkl', 'wb') as f:\n    pickle.dump(tokenizer,f)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T00:12:00.996796Z","iopub.execute_input":"2024-04-25T00:12:00.997710Z","iopub.status.idle":"2024-04-25T00:12:01.019230Z","shell.execute_reply.started":"2024-04-25T00:12:00.997674Z","shell.execute_reply":"2024-04-25T00:12:01.017870Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"from rouge import Rouge\n\nrouge = Rouge()\n\n\ndef calculate_rouge(row):\n    return rouge.get_scores(row['predicted_summary'], row['Summary'], avg=True)\n\ndf_train['rouge'] = df_train.apply(calculate_rouge, axis=1)\ndf_train['rouge-1'] = df_train['rouge'].apply(lambda x: x['rouge-1']['f'])\ndf_train['rouge-2'] = df_train['rouge'].apply(lambda x: x['rouge-2']['f'])\n\nprint(df_train['rouge-1'].mean())\nprint(df_train['rouge-2'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-04-25T00:08:54.677253Z","iopub.execute_input":"2024-04-25T00:08:54.677581Z","iopub.status.idle":"2024-04-25T00:08:54.883987Z","shell.execute_reply.started":"2024-04-25T00:08:54.677553Z","shell.execute_reply":"2024-04-25T00:08:54.882440Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/113450990.py:9: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_train['rouge'] = df_train.apply(calculate_rouge, axis=1)\n","output_type":"stream"},{"name":"stdout","text":"0.5612480810776703\n0.45123890217177354\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/113450990.py:10: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_train['rouge-1'] = df_train['rouge'].apply(lambda x: x['rouge-1']['f'])\n/tmp/ipykernel_34/113450990.py:11: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_train['rouge-2'] = df_train['rouge'].apply(lambda x: x['rouge-2']['f'])\n","output_type":"stream"}]},{"cell_type":"code","source":"# # Define compute_metrics function\n# def compute_metrics(eval_pred):\n#     predictions, labels = eval_pred\n#     decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n#     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n#     # Load ROUGE and BERT score metrics\n#     rouge = load_metric('rouge')\n#     bert_score = load_metric('bertscore')\n\n#     # Compute ROUGE and BERT scores\n#     rouge_output = rouge.compute(predictions=decoded_preds, references=decoded_labels, rouge_types=[\"rougeL\", \"rouge2\"])\n#     bert_score_output = bert_score.compute(predictions=decoded_preds, references=decoded_labels, lang=\"hi\")\n\n#     return {\n#         \"rouge2\": rouge_output['rouge2'].mid.fmeasure,\n#         \"rougeL\": rouge_output['rougeL'].mid.fmeasure,\n#         \"bert_score\": bert_score_output['f1'][0]\n#     }","metadata":{"execution":{"iopub.status.busy":"2024-04-25T00:08:54.885753Z","iopub.execute_input":"2024-04-25T00:08:54.886811Z","iopub.status.idle":"2024-04-25T00:08:54.893630Z","shell.execute_reply.started":"2024-04-25T00:08:54.886764Z","shell.execute_reply":"2024-04-25T00:08:54.892259Z"},"trusted":true},"execution_count":44,"outputs":[]}]}