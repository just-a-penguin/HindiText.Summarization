{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8204485,"sourceType":"datasetVersion","datasetId":4861026},{"sourceId":8218864,"sourceType":"datasetVersion","datasetId":4871900}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict\ndataset = load_dataset('csv', data_files='/kaggle/input/2022-data/train.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:01:58.005414Z","iopub.execute_input":"2024-04-25T09:01:58.005836Z","iopub.status.idle":"2024-04-25T09:01:58.166475Z","shell.execute_reply.started":"2024-04-25T09:01:58.005802Z","shell.execute_reply":"2024-04-25T09:01:58.165680Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nimport torch\nimport torch.nn.functional as F\n\n# google/mt5-small\n# Define the student model\nstudent_model = AutoModelForSeq2SeqLM.from_pretrained(\"ai4bharat/IndicBART\")\nstudent_tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/IndicBART\")\n\n# Define the teacher models\nteacher_model1 = AutoModelForSeq2SeqLM.from_pretrained(\"ai4bharat/IndicBART\")\n# teacher_model_2 = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-base\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:01:58.168294Z","iopub.execute_input":"2024-04-25T09:01:58.168672Z","iopub.status.idle":"2024-04-25T09:02:00.642977Z","shell.execute_reply.started":"2024-04-25T09:01:58.168638Z","shell.execute_reply":"2024-04-25T09:02:00.641891Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Define the loss function\ndef distillation_loss(student_outputs, labels, teacher_outputs_1, teacher_outputs_2, alpha_ce=0.5, alpha_distil=0.5):\n    cross_entropy_loss = F.cross_entropy(student_outputs.logits.view(-1, student_outputs.logits.size(-1)), labels.view(-1))\n#     if student_outputs.logits.size(-1) < teacher_outputs_1.logits.size(-1):\n#     # Pad student logits with zeros (assuming teacher has more classes)\n#         padding_size = teacher_outputs_1.logits.size(-1) - student_outputs.logits.size(-1)\n#         student_outputs.logits = torch.nn.functional.pad(student_outputs.logits, (0, padding_size))\n\n    distillation_loss_1 = F.kl_div(\n        F.log_softmax(student_outputs.logits / 2.0, dim=-1),\n        F.softmax(teacher_outputs_1.logits / 2.0, dim=-1),\n        reduction='batchmean'\n    )\n\n#     distillation_loss_2 = F.kl_div(\n#         F.log_softmax(student_outputs.logits / 2.0, dim=-1),\n#         F.softmax(teacher_outputs_2.logits / 2.0, dim=-1),\n#         reduction='batchmean'\n#     )\n    distillation_loss_2 =0\n\n    distillation_loss = (distillation_loss_1 + distillation_loss_2) / 2.0\n\n    total_loss = alpha_ce * cross_entropy_loss + alpha_distil * distillation_loss\n\n    return total_loss","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:02:00.644721Z","iopub.execute_input":"2024-04-25T09:02:00.645352Z","iopub.status.idle":"2024-04-25T09:02:00.653762Z","shell.execute_reply.started":"2024-04-25T09:02:00.645321Z","shell.execute_reply":"2024-04-25T09:02:00.652800Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# def distillation_loss(student_outputs, labels, teacher_outputs_1, teacher_outputs_2=None, alpha_ce=0.5, alpha_distil=0.5):\n#   \"\"\"\n#   Computes the combined loss for knowledge distillation.\n\n#   Args:\n#       student_outputs: A dictionary containing the student model's outputs.\n#           Expected to have a key 'logits' for the logits tensor.\n#       labels: A tensor containing the ground truth labels.\n#       teacher_outputs_1: A dictionary containing the first teacher model's outputs.\n#           Expected to have a key 'logits' for the logits tensor.\n#       teacher_outputs_2 (optional): A dictionary containing the second teacher model's outputs (if using two teachers).\n#           Expected to have a key 'logits' for the logits tensor. Defaults to None.\n#       alpha_ce: Weight for the cross-entropy loss (default: 0.5).\n#       alpha_distil: Weight for the distillation loss (default: 0.5).\n\n#   Returns:\n#       A tensor representing the combined loss.\n#   \"\"\"\n\n#   cross_entropy_loss = F.cross_entropy(student_outputs.logits.view(-1, student_outputs.logits.size(-1)), labels.view(-1))\n\n#   # Handle potential size mismatch between student and teacher logits\n#   if student_outputs.logits.size(-1) < teacher_outputs_1.logits.size(-1):\n#       # Pad student logits with zeros (assuming teacher has more classes)\n#     padding_size = teacher_outputs_1.logits.size(-1) - student_outputs.logits.size(-1)\n#     student_outputs.logits = torch.nn.functional.pad(student_outputs.logits, (0, padding_size))\n    \n#   softL=F.log_softmax(student_outputs.logits / 2.0, dim=-1)\n#   TeachL=F.softmax(teacher_outputs_1.logits / 2.0, dim=-1)\n#   if softL.size(-1) < TeachL.size(-1):\n    \n#     padding_size = TeachL.size(-1) - softL.size(-1)\n#     softL = torch.nn.functional.pad(softL, (0, padding_size))\n    \n#   if TeachL.size(-1) < softL.size(-1):\n    \n#     padding_size =- TeachL.size(-1) +softL.size(-1)\n#     TeachL = torch.nn.functional.pad(TeachL, (0, padding_size))\n#   print(softL.size(-1),TeachL.size(-1))\n#   print(softL.shape,TeachL.shape)\n\n#   distillation_loss_1 = F.kl_div(\n#       softL,\n#       TeachL,\n#       reduction='batchmean'\n#   )\n\n#   # Handle second teacher model if provided (assuming same logic for size mismatch)\n#   distillation_loss_2 = 0\n#   if teacher_outputs_2 is not None:\n#       if student_outputs.logits.size(-1) < teacher_outputs_2.logits.size(-1):\n#           padding_size = teacher_outputs_2.logits.size(-1) - student_outputs.logits.size(-1)\n#           student_outputs.logits = torch.nn.functional.pad(student_outputs.logits, (0, padding_size))\n#       distillation_loss_2 = F.kl_div(\n#           F.log_softmax(student_outputs.logits / 2.0, dim=-1),\n#           F.softmax(teacher_outputs_2.logits / 2.0, dim=-1),\n#           reduction='batchmean'\n#       )\n\n#   distillation_loss = (distillation_loss_1 + distillation_loss_2) / 2.0\n\n#   total_loss = alpha_ce * cross_entropy_loss + alpha_distil * distillation_loss\n\n#   return total_loss\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:02:00.659114Z","iopub.execute_input":"2024-04-25T09:02:00.659543Z","iopub.status.idle":"2024-04-25T09:02:00.669301Z","shell.execute_reply.started":"2024-04-25T09:02:00.659510Z","shell.execute_reply":"2024-04-25T09:02:00.668340Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"teacher_tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/IndicBART\")","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:02:00.670605Z","iopub.execute_input":"2024-04-25T09:02:00.671382Z","iopub.status.idle":"2024-04-25T09:02:01.546158Z","shell.execute_reply.started":"2024-04-25T09:02:00.671350Z","shell.execute_reply":"2024-04-25T09:02:01.545084Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom datasets import load_dataset, DatasetDict\ndataset = load_dataset('csv', data_files='/kaggle/input/2022-data/train.csv')\n\nmax_input_length = 100\nmax_target_length = 50\n# Load the tokenizer for the student model\nprefix = 'summarize'\n\n# Define the encoding function\ndef encode_data(examples):\n    inputs = [prefix + doc for doc in examples[\"Article\"]]\n    model_inputs = student_tokenizer(inputs, max_length=max_input_length, truncation=True,padding=True)\n\n    # Setup the tokenizer for targets\n    labels = student_tokenizer(text_target=examples[\"Summary\"], max_length=max_target_length, padding=True)\n#     labels = tokenizer(text_target=examples[\"Summary\"], padding='longest', truncation=True)\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Define the encoding function\ndef encode_data2(examples):\n    inputs = [prefix + doc for doc in examples[\"Article\"]]\n    model_inputs = teacher_tokenizer(inputs, max_length=max_input_length, truncation=True,padding=True)\n\n    # Setup the tokenizer for targets\n    labels = teacher_tokenizer(text_target=examples[\"Summary\"], max_length=max_target_length, padding=True)\n#     labels = tokenizer(text_target=examples[\"Summary\"], padding='longest', truncation=True)\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# Apply the encoding function to the dataset\nencoded_dataset = dataset['train'].map(encode_data, batched=True, remove_columns=['Article', 'Summary','Heading','id'])\nencoded_dataset2 = dataset['train'].map(encode_data2, batched=True, remove_columns=['Article', 'Summary','Heading','id'])","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:02:01.547847Z","iopub.execute_input":"2024-04-25T09:02:01.548514Z","iopub.status.idle":"2024-04-25T09:02:01.717596Z","shell.execute_reply.started":"2024-04-25T09:02:01.548484Z","shell.execute_reply":"2024-04-25T09:02:01.716584Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"encoded_dataset","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:02:01.718853Z","iopub.execute_input":"2024-04-25T09:02:01.719154Z","iopub.status.idle":"2024-04-25T09:02:01.725393Z","shell.execute_reply.started":"2024-04-25T09:02:01.719130Z","shell.execute_reply":"2024-04-25T09:02:01.724377Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n    num_rows: 7957\n})"},"metadata":{}}]},{"cell_type":"code","source":"encoded_dataset2","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:02:01.726649Z","iopub.execute_input":"2024-04-25T09:02:01.727005Z","iopub.status.idle":"2024-04-25T09:02:01.743027Z","shell.execute_reply.started":"2024-04-25T09:02:01.726968Z","shell.execute_reply":"2024-04-25T09:02:01.742066Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n    num_rows: 7957\n})"},"metadata":{}}]},{"cell_type":"code","source":"\nsubset_dataset1 = encoded_dataset\n# subset_size = 1000  # Specify the size of the subset you want to create\n\n# subset_dataset1 = dataset_dict.select(range(subset_size))\nsubset_dataset_dict = DatasetDict({'train': subset_dataset1})\nencoded_dataset=subset_dataset_dict","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:02:01.744457Z","iopub.execute_input":"2024-04-25T09:02:01.744796Z","iopub.status.idle":"2024-04-25T09:02:01.758060Z","shell.execute_reply.started":"2024-04-25T09:02:01.744771Z","shell.execute_reply":"2024-04-25T09:02:01.756926Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"\nsubset_dataset1 = encoded_dataset2\nsubset_size = 1000  # Specify the size of the subset you want to create\n\n# subset_dataset1 = dataset_dict.select(range(subset_size))\nsubset_dataset_dict = DatasetDict({'train': subset_dataset1})\nencoded_dataset2=subset_dataset_dict","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:02:01.761368Z","iopub.execute_input":"2024-04-25T09:02:01.761750Z","iopub.status.idle":"2024-04-25T09:02:01.773577Z","shell.execute_reply.started":"2024-04-25T09:02:01.761725Z","shell.execute_reply":"2024-04-25T09:02:01.772676Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"encoded_dataset","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:02:01.775180Z","iopub.execute_input":"2024-04-25T09:02:01.775776Z","iopub.status.idle":"2024-04-25T09:02:01.791441Z","shell.execute_reply.started":"2024-04-25T09:02:01.775744Z","shell.execute_reply":"2024-04-25T09:02:01.790509Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n        num_rows: 7957\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"encoded_dataset = encoded_dataset2.remove_columns('token_type_ids')","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:02:01.792589Z","iopub.execute_input":"2024-04-25T09:02:01.793413Z","iopub.status.idle":"2024-04-25T09:02:01.800923Z","shell.execute_reply.started":"2024-04-25T09:02:01.793381Z","shell.execute_reply":"2024-04-25T09:02:01.799961Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"encoded_dataset2 = encoded_dataset2.remove_columns('token_type_ids')","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:02:01.802460Z","iopub.execute_input":"2024-04-25T09:02:01.802841Z","iopub.status.idle":"2024-04-25T09:02:01.813464Z","shell.execute_reply.started":"2024-04-25T09:02:01.802807Z","shell.execute_reply":"2024-04-25T09:02:01.812448Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"encoded_dataset2","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:02:01.814804Z","iopub.execute_input":"2024-04-25T09:02:01.815179Z","iopub.status.idle":"2024-04-25T09:02:01.824333Z","shell.execute_reply.started":"2024-04-25T09:02:01.815147Z","shell.execute_reply":"2024-04-25T09:02:01.823288Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 7957\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# inputs = {\n#         'input_ids': batch['input_ids'],\n#         'attention_mask': batch['attention_mask'],\n#         'labels': batch['labels']\n#     }","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:07:13.240548Z","iopub.execute_input":"2024-04-25T09:07:13.240978Z","iopub.status.idle":"2024-04-25T09:07:13.245500Z","shell.execute_reply.started":"2024-04-25T09:07:13.240947Z","shell.execute_reply":"2024-04-25T09:07:13.244444Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom transformers import DataCollatorForSeq2Seq\n\n\ndata_collator1 = DataCollatorForSeq2Seq(tokenizer=student_tokenizer)\ndata_collator2 = DataCollatorForSeq2Seq(tokenizer=teacher_tokenizer)\n\ntrain_dataloader1 = DataLoader(encoded_dataset['train'], batch_size=2, shuffle=True, collate_fn=data_collator1)\ntrain_dataloader2 = DataLoader(encoded_dataset2['train'], batch_size=2, shuffle=True, collate_fn=data_collator2)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:07:13.251764Z","iopub.execute_input":"2024-04-25T09:07:13.252084Z","iopub.status.idle":"2024-04-25T09:07:13.258685Z","shell.execute_reply.started":"2024-04-25T09:07:13.252033Z","shell.execute_reply":"2024-04-25T09:07:13.257612Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"len(train_dataloader1)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:07:13.263075Z","iopub.execute_input":"2024-04-25T09:07:13.263451Z","iopub.status.idle":"2024-04-25T09:07:13.272250Z","shell.execute_reply.started":"2024-04-25T09:07:13.263416Z","shell.execute_reply":"2024-04-25T09:07:13.271278Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"3979"},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom tqdm import tqdm  # Optional: for progress bar\n\n\noptimizer = torch.optim.Adam(student_model.parameters())\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model.to(device)\n# Assuming you have two pre-defined DataLoaders: train_dataloader1 and train_dataloader2\nteacher_model1.to(device)\nstudent_model.to(device)\ni=0\n# max_iters = min(len(train_dataloader1), len(train_dataloader2))  # Synchronize iterations\niterator1 = iter(train_dataloader1)\niterator2 = iter(train_dataloader2)\nmax_iters=1990\nfor step in tqdm(range(max_iters)):\n    torch.cuda.empty_cache()\n    try:\n        inputs1 = next(iterator1)\n        inputs2 = next(iterator2)\n    except StopIteration:\n        # Handle reaching the end of a DataLoader (optional, e.g., reset iterators)\n        iterator1 = iter(train_dataloader1)  # Consider resetting\n        iterator2 = iter(train_dataloader2)  # Consider resetting\n#         inputs1 = next(iterator1).to(device)\n#         inputs2 = next(iterator2).to(device)\n    \n\n    # Ensure 'input_ids' (or relevant key) is a tensor (if necessary)\n    if isinstance(inputs1['input_ids'], list):\n        inputs1['input_ids'] = torch.tensor(inputs1['input_ids'])  # Convert to tensor\n    if isinstance(inputs2['input_ids'], list):\n        inputs2['input_ids'] = torch.tensor(inputs2['input_ids'])  # Convert to tensor\n\n    # Select teacher model (replace with your selection logic)\n#     teacher_model = teacher_model_1 if step % 2 == 0 else teacher_model_2\n    inputs1.to(device)\n    inputs2.to(device)\n    # Forward pass through student and teacher models\n    student_outputs = student_model(**inputs1)\n    teacher_outputs = teacher_model1(**inputs2)\n#     print(student_outputs.logits.shape)\n#     print(teacher_outputs.logits.shape)\n    if(student_outputs.logits.shape!=teacher_outputs.logits.shape):\n        i+=1\n        continue\n    # Compute the distillation loss\n    loss = distillation_loss(student_outputs, inputs1['labels'], teacher_outputs,0)\n\n    # Perform backpropagation\n    loss.backward()\n\n    # Update the weights\n    optimizer.step()\n    optimizer.zero_grad()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:07:41.723712Z","iopub.execute_input":"2024-04-25T09:07:41.724100Z","iopub.status.idle":"2024-04-25T09:12:46.691548Z","shell.execute_reply.started":"2024-04-25T09:07:41.724065Z","shell.execute_reply":"2024-04-25T09:12:46.690682Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stderr","text":"100%|██████████| 1990/1990 [05:04<00:00,  6.53it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import pickle\n\nwith open('indicbart_distil.pkl','wb') as f:\n   pickle.dump(student_model, f) \n    \n# with open('mt5-base.pkl', 'wb') as f:\n#     pickle.dump(model, f)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:19:19.445596Z","iopub.execute_input":"2024-04-25T09:19:19.446343Z","iopub.status.idle":"2024-04-25T09:19:21.307551Z","shell.execute_reply.started":"2024-04-25T09:19:19.446311Z","shell.execute_reply":"2024-04-25T09:19:21.306680Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"with open('indicbart_distil_tokenizer.pkl', 'wb') as f:\n    pickle.dump(student_tokenizer, f)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:19:30.788671Z","iopub.execute_input":"2024-04-25T09:19:30.789403Z","iopub.status.idle":"2024-04-25T09:19:30.806049Z","shell.execute_reply.started":"2024-04-25T09:19:30.789371Z","shell.execute_reply":"2024-04-25T09:19:30.805001Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"!pip install rouge","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:17:06.503446Z","iopub.execute_input":"2024-04-25T09:17:06.503999Z","iopub.status.idle":"2024-04-25T09:17:21.069760Z","shell.execute_reply.started":"2024-04-25T09:17:06.503957Z","shell.execute_reply":"2024-04-25T09:17:21.068575Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge\n  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge) (1.16.0)\nDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\nInstalling collected packages: rouge\nSuccessfully installed rouge-1.0.1\n","output_type":"stream"}]},{"cell_type":"code","source":"with open('/kaggle/working/indicbart_distil.pkl', 'rb') as f:\n    model = pickle.load(f)\n\nwith open('/kaggle/working/indicbart_distil_tokenizer.pkl', 'rb') as f:\n    tokenizer = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:21:26.041071Z","iopub.execute_input":"2024-04-25T09:21:26.041465Z","iopub.status.idle":"2024-04-25T09:21:27.369325Z","shell.execute_reply.started":"2024-04-25T09:21:26.041434Z","shell.execute_reply":"2024-04-25T09:21:27.368361Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom rouge import Rouge\ndf_test=pd.read_csv(\"/kaggle/input/2022-data/test.csv\")\ndf_test=df_test[:10]\ndef predict_summary(text):\n    inputs = tokenizer(text, return_tensors='pt', max_length=600, truncation=True)\n    inputs.to(\"cuda\")\n    summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=40, early_stopping=True)\n    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    return summary\n\ndf_test['predicted_summary'] = df_test['Article'].apply(predict_summary)\n\nrouge = Rouge()\n\n\ndef calculate_rouge(row):\n    return rouge.get_scores(row['predicted_summary'], row['Summary'], avg=True)\n\ndf_test['rouge'] = df_test.apply(calculate_rouge, axis=1)\ndf_test['rouge-1'] = df_test['rouge'].apply(lambda x: x['rouge-1']['f'])\ndf_test['rouge-2'] = df_test['rouge'].apply(lambda x: x['rouge-2']['f'])\n\nprint(df_test['rouge-1'].mean())\nprint(df_test['rouge-2'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:21:49.899453Z","iopub.execute_input":"2024-04-25T09:21:49.900262Z","iopub.status.idle":"2024-04-25T09:21:54.552865Z","shell.execute_reply.started":"2024-04-25T09:21:49.900228Z","shell.execute_reply":"2024-04-25T09:21:54.551849Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"0.5484455285199256\n0.43516804200716497\n","output_type":"stream"}]},{"cell_type":"code","source":"print(i)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:25:18.347877Z","iopub.execute_input":"2024-04-25T09:25:18.348293Z","iopub.status.idle":"2024-04-25T09:25:18.357103Z","shell.execute_reply.started":"2024-04-25T09:25:18.348240Z","shell.execute_reply":"2024-04-25T09:25:18.355853Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"1588\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.optim import AdamW\n\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\nfrom accelerate import Accelerator\n\naccelerator = Accelerator()\nmodel, optimizer, train_dataloader = accelerator.prepare(\n    model, optimizer, train_dataloader1\n)\n\nfrom transformers import get_scheduler\n\nnum_train_epochs = 1\nnum_update_steps_per_epoch = len(train_dataloader)\nnum_training_steps = num_train_epochs * num_update_steps_per_epoch\n\nlr_scheduler = get_scheduler(\n    \"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps,\n)\n\nimport nltk\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [label.strip() for label in labels]\n\n    # ROUGE expects a newline after each sentence\n    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n\n    return preds, labels","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:30:35.070140Z","iopub.execute_input":"2024-04-25T09:30:35.070785Z","iopub.status.idle":"2024-04-25T09:30:35.625866Z","shell.execute_reply.started":"2024-04-25T09:30:35.070750Z","shell.execute_reply":"2024-04-25T09:30:35.623817Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:31:30.320502Z","iopub.execute_input":"2024-04-25T09:31:30.321485Z","iopub.status.idle":"2024-04-25T09:31:45.621731Z","shell.execute_reply.started":"2024-04-25T09:31:30.321438Z","shell.execute_reply":"2024-04-25T09:31:45.620550Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=774bcaa0537c245378f74309fe869035d08e6b5548cc3056e9e47afbe62fd23e\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"from tqdm.auto import tqdm\nimport torch\nimport numpy as np\n\nfrom datasets import load_metric\n\n# Load ROUGE metric\nrouge_metric = load_metric(\"rouge\")\n\nprogress_bar = tqdm(range(num_training_steps))\n# all_logits = []\n\nfor epoch in range(num_train_epochs):\n    # Training\n    model.train()\n    torch.cuda.empty_cache()\n    for step, batch in enumerate(train_dataloader):\n        outputs = model(**batch)\n        logits = outputs.logits\n#         all_logits.append(logits.detach().cpu().numpy())\n        loss = outputs.loss\n        accelerator.backward(loss)\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)\n\n    # Evaluation\n#     model.eval()\n#     for step, batch in enumerate(eval_dataloader):\n#         with torch.no_grad():\n#             generated_tokens = accelerator.unwrap_model(model).generate(\n#                 batch[\"input_ids\"],\n#                 attention_mask=batch[\"attention_mask\"],\n#             )\n\n#             generated_tokens = accelerator.pad_across_processes(\n#                 generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n#             )\n#             labels = batch[\"labels\"]\n\n#             # If we did not pad to max length, we need to pad the labels too\n#             labels = accelerator.pad_across_processes(\n#                 batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id\n#             )\n\n#             generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n#             labels = accelerator.gather(labels).cpu().numpy()\n\n#             # Replace -100 in the labels as we can't decode them\n#             labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n#             if isinstance(generated_tokens, tuple):\n#                 generated_tokens = generated_tokens[0]\n#             decoded_preds = tokenizer.batch_decode(\n#                 generated_tokens, skip_special_tokens=True\n#             )\n#             decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n#             decoded_preds, decoded_labels = postprocess_text(\n#                 decoded_preds, decoded_labels\n#             )\n\n#             rouge_metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n\n#     # Compute metrics\n#     result = rouge_metric.compute()\n#     # Extract the median ROUGE scores\n#     result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n#     result = {k: round(v, 4) for k, v in result.items()}\n#     print(f\"Epoch {epoch}:\", result)\n\n#     # Save and upload\n#     accelerator.wait_for_everyone()\n#     unwrapped_model = accelerator.unwrap_model(model)\n#     unwrapped_model.save_pretrained('indicbart', save_function=accelerator.save)\n#     if accelerator.is_main_process:\n#         tokenizer.save_pretrained('indicbart_tokeniser')\n#         repo.push_to_hub(\n#             commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n#         )","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:27:41.314152Z","iopub.execute_input":"2024-04-25T10:27:41.315014Z","iopub.status.idle":"2024-04-25T10:41:11.188711Z","shell.execute_reply.started":"2024-04-25T10:27:41.314976Z","shell.execute_reply":"2024-04-25T10:41:11.187295Z"},"trusted":true},"execution_count":82,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/rouge/rouge.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3979 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d94883505b294eddafcbf40983ec4083"}},"metadata":{}}]},{"cell_type":"code","source":"import pickle\n\nwith open('indicbart_distil_train.pkl','wb') as f:\n   pickle.dump(student_model, f) \n\nwith open('indicbart_distil_tokenizer_train.pkl', 'wb') as f:\n    pickle.dump(student_tokenizer, f)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:57:50.412255Z","iopub.execute_input":"2024-04-25T09:57:50.413106Z","iopub.status.idle":"2024-04-25T09:57:53.079395Z","shell.execute_reply.started":"2024-04-25T09:57:50.413071Z","shell.execute_reply":"2024-04-25T09:57:53.078186Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom rouge import Rouge\ndf_test=pd.read_csv(\"/kaggle/input/2022-data/test.csv\")\n# df_test=df_test[:20]\ndef predict_summary(text):\n    inputs = tokenizer(text, return_tensors='pt', max_length=600, truncation=True)\n    inputs.to(\"cuda\")\n    summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=40, early_stopping=True)\n    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n    return summary\n\ndf_test['predicted_summary'] = df_test['Article'].apply(predict_summary)\n\nrouge = Rouge()\n\n\ndef calculate_rouge(row):\n    return rouge.get_scores(row['predicted_summary'], row['Summary'], avg=True)\n\ndf_test['rouge'] = df_test.apply(calculate_rouge, axis=1)\ndf_test['rouge-1'] = df_test['rouge'].apply(lambda x: x['rouge-1']['f'])\ndf_test['rouge-2'] = df_test['rouge'].apply(lambda x: x['rouge-2']['f'])\n\nprint(df_test['rouge-1'].mean())\nprint(df_test['rouge-2'].mean())","metadata":{"execution":{"iopub.status.busy":"2024-04-25T10:42:28.721290Z","iopub.execute_input":"2024-04-25T10:42:28.722170Z","iopub.status.idle":"2024-04-25T11:03:47.887032Z","shell.execute_reply.started":"2024-04-25T10:42:28.722136Z","shell.execute_reply":"2024-04-25T11:03:47.885763Z"},"trusted":true},"execution_count":85,"outputs":[{"name":"stdout","text":"0.4747669689209107\n0.34715083883702935\n","output_type":"stream"}]},{"cell_type":"code","source":"\nfrom transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./student_model\",\n    num_train_epochs=1,\n    per_device_train_batch_size=8,\n    save_steps=10_000,\n    save_total_limit=2,\n    logging_dir=\"./logs\",\n)\n\ntrainer = Trainer(\n    model=student_model,\n    args=training_args,\n    train_dataset=encoded_dataset['train'],\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:23:17.911749Z","iopub.execute_input":"2024-04-25T09:23:17.912629Z","iopub.status.idle":"2024-04-25T09:25:06.941306Z","shell.execute_reply.started":"2024-04-25T09:23:17.912590Z","shell.execute_reply":"2024-04-25T09:25:06.938740Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240425_092448-0yxat848</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/iiitd-ed/huggingface/runs/0yxat848' target=\"_blank\">fanciful-music-44</a></strong> to <a href='https://wandb.ai/iiitd-ed/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/iiitd-ed/huggingface' target=\"_blank\">https://wandb.ai/iiitd-ed/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/iiitd-ed/huggingface/runs/0yxat848' target=\"_blank\">https://wandb.ai/iiitd-ed/huggingface/runs/0yxat848</a>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[66], line 40\u001b[0m\n\u001b[1;32m     25\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     26\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./student_model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     logging_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./logs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     34\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     35\u001b[0m     model\u001b[38;5;241m=\u001b[39mstudent_model,\n\u001b[1;32m     36\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     37\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mencoded_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     38\u001b[0m )\n\u001b[0;32m---> 40\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2085\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2082\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2084\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2085\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   2086\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2088\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/data_loader.py:452\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 452\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/data/data_collator.py:92\u001b[0m, in \u001b[0;36mdefault_data_collator\u001b[0;34m(features, return_tensors)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# In this function we'll make the assumption that all `features` in the batch\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# have the same attributes.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# So we will look at the first element as a proxy for what attributes exist\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# on the whole batch.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_default_data_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf_default_data_collator(features)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/data/data_collator.py:158\u001b[0m, in \u001b[0;36mtorch_default_data_collator\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m    156\u001b[0m             batch[k] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39mstack([f[k] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]))\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m             batch[k] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n","\u001b[0;31mValueError\u001b[0m: expected sequence of length 110 at dim 1 (got 114)"],"ename":"ValueError","evalue":"expected sequence of length 110 at dim 1 (got 114)","output_type":"error"}]},{"cell_type":"code","source":"# # Prepare the data\n# inputs = ...  # Prepare the input data\n# labels = ...  # Prepare the ground truth labels\n\n# # Forward pass through the student model\n# student_outputs = student_model(**inputs, labels=labels)\n\n# # Forward pass through the teacher models\n# with torch.no_grad():\n#     teacher_outputs_1 = teacher_model_1(**inputs, labels=labels)\n#     teacher_outputs_2 = teacher_model_2(**inputs, labels=labels)\n\n# # Calculate the distillation loss\n# loss = distillation_loss(student_outputs, labels, teacher_outputs_1, teacher_outputs_2)\n\n# # Backward pass and optimization\n# loss.backward()\n# optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2024-04-25T09:07:19.331620Z","iopub.status.idle":"2024-04-25T09:07:19.332021Z","shell.execute_reply.started":"2024-04-25T09:07:19.331832Z","shell.execute_reply":"2024-04-25T09:07:19.331855Z"},"trusted":true},"execution_count":null,"outputs":[]}]}